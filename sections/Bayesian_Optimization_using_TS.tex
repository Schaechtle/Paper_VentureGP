We introduce Thompson sampling, the basic solution strategy
underlying the Bayesian optimization with \gpmem.
Thompson sampling~\cite{thompson1933likelihood} is a widely used Bayesian
framework for addressing the trade-off between exploration and exploitation in
multi-armed (or continuum-armed) bandit problems.  
We cast the multi-armed bandit problem as a one-state Markov
decision process, and describe how Thompson sampling can be used to choose
actions for that \ac{MDP}.

Here, an agent is to take a sequence of actions $a_1, a_2,
\ldots$ from a (possibly infinite) set of possible actions $\Acal$.  After each
action, a reward $r \in \R$ is received, according to an unknown conditional
distribution $P_\true\pn{r \mvert a}$.  The agent's goal is to maximize the
total reward received for all actions in an online manner.  In Thompson
sampling, the agent accomplishes this by placing a prior distribution
$P(\ttheta)$ on the possible ``contexts'' $\ttheta \in \Theta$.  Here a context
is a believed model of the conditional distributions $\{P\pn{r \mvert a}\}_{a
\in \Acal}$, or at least, a believed statistic of these conditional
distributions which is sufficient for deciding an action $a$.  If actions are
chosen so as to maximize expected reward, then one such sufficient statistic is
the believed conditional mean $V\pn{a \mvert \ttheta} = \Ebkt{r \mvert
a;\ttheta}$, which can be viewed as a believed value function.  For
consistency with what follows, we will assume our context $\ttheta$ takes the
form $(\theta, \abf_\past, \rbf_\past)$ where $\abf_\past$ is the vector of past
actions, $\rbf_\past$ is the vector of their rewards, and $\theta$ (the
``semicontext'') contains any other information that is included in the context.

In this setup, Thompson sampling has the following steps:
\begin{algorithm}[H]
  \singlespacing
  Repeat as long as desired:
  \begin{enumerate}
    \item\label{itm:thompson-step-sample} {\bf Sample.} Sample a semicontext
      $\theta \sim P(\theta)$.
    \item\label{itm:thompson-step-search} {\bf Search (and act).} Choose an
      action $a \in \Acal$ which (approximately) maximizes $V\pn{a \mvert
      \ttheta} = \Ebkt{r \mvert a; \ttheta} = \Ebkt{r \mvert a;\,\theta,
      \abf_\past, \rbf_\past}$.
    \item {\bf Update.} Let $r_\true$ be the reward received for action $a$.
      Update the believed distribution on $\theta$, i.e., $P(\theta) \gets
      P_\rmnew(\theta)$ where $P_\rmnew(\theta) = P\pn{\theta \mvert a \mapsto
      r_\true}$.
  \end{enumerate}
  \caption{Thompson sampling.}
  \label{alg:thompson}
\end{algorithm}
Note that when $\Ebkt{r|a;\ttheta}$ (under the sampled value of $\theta$ for
some points $a$) is far from the true value $\Ebkt[P_\true]{r \mvert a}$, the
chosen action $a$ may be far from optimal, but the information gained by probing
action $a$ will improve the belief $\ttheta$.  This amounts to ``exploration.''
When $\Ebkt{r \mvert a;\ttheta}$ is close to the true value except at points $a$
for which $\Ebkt{r \mvert a;\ttheta}$ is low, exploration will be less likely to
occur, but the chosen actions $a$ will tend to receive high rewards.  This
amounts to ``exploitation.'' The trade-off between exploration and exploitation
is illustrated in Figure \ref{fig:slide2}.  Roughly speaking, exploration will
happen until the context $\ttheta$ is reasonably sure that the unexplored
actions are probably not optimal, at which time the Thompson sampler will
exploit by choosing actions in regions it knows to have high value.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figs/slide2.pdf}
  \caption{
    Two possible actions (in green) for an iteration of Thompson sampling.  The
    believed distribution on the value function $V$ is depicted in red.  In this
    example, the true reward function is deterministic, and is drawn in blue.
    The action on the right receives a high reward, while the action on the left
    receives a low reward but greatly improves the accuracy of the believed
    distribution on $V$.  The transition operators $\tau_\search$ and
    $\tau_\update$ are described in Section \ref{sec:math-spec}.
  }
  \label{fig:slide2}
\end{figure}

Typically, when Thompson sampling is implemented, the search over contexts
$\ttheta \in \Theta$ is limited by the choice of representation.  In
traditional programming environments, $\theta$ often consists of a few
numerical parameters for a family of distributions of a fixed functional
form.  With work, a mixture of a few functional forms is possible; but
without probabilistic programming machinery, implementing a rich context
space $\Theta$ would be an unworkably large technical burden.  In a
probabilstic programing language, however, the representation of
heterogeneously structured or infinite-dimensional context spaces is quite
natural.  Any computable model of the conditional distributions
$\br{P\pn{r \mvert a}}_{a \in \Acal}$ can be represented as a stochastic
procedure $(\lambda (a) \ldots)$.  Thus, for computational Thompson sampling,
the most general context space $\widehat\Theta$ is the space of program texts.
Any other context space $\Theta$ has a natural embedding as a subset of
$\widehat\Theta$.
\begin{comment}
\myparagraph{Thompson Sampling with a Statistical Memoizer}
\label{sec:thompson-mem-em}
Thompson sampling as described above can be expressed compactly in terms of a
statistical memoizer, as in Listing \ref{lst:thompson-mem-em} (see also Figure
\ref{fig:slide1}).  Here and in what follows, to simplify the treatment, we
assume the true reward function is deterministic, i.e., for each fixed $a$, the
conditional distribution $P_\true\pn{r \mvert a}$ is a delta distribution.  We
thus treat the notations $a,r,\abf_\past,\rbf_\past$ as synonyms for
$x,y,\xbf_\past,\ybf_\past$, respectively, differing only in that they carry the
connotations of ``action'' and ``reward.''

\begin{mdframed}
\begin{minipage}{\linewidth}
\small
\belowcaptionskip=-10pt
\begin{lstlisting}[escapechar=\#,language=Venture,label=lst:thompson-mem-em,caption={
  Code template for Thompson sampling in pseudo-Venture using a statistical
  memoizer.  The choice of statistical memoizer (\texttt{mem\&em}), prior on
  semicontexts (\texttt{prior\_on\_semicontexts}), and approximation strategy
  for maximizing the emulator (\texttt{argmax}) are not included.
}]
#\linenumber{1}#assume theta = tag(quote( theta ), 0,  prior_on_semicontexts())
#\linenumber{2}#assume (r_probe r_emulate) = mem&em( do_action, theta)

#\linenumber{3}#infer repeat(15, do(pass,
#\linenumber{4}#     a = mc_argmax(r_emulate, quote(_)),
#\linenumber{5}#     r_probe(a)
#\linenumber{6}#     mh(quote(theta), one, 50)))
\end{lstlisting}
\end{minipage}
\end{mdframed}

\FloatBarrier

In Listing \ref{lst:thompson-mem-em}, \texttt{do\_action} is a procedure which
interfaces with the outside environment and returns the reward received.
\texttt{do\_action} is wrapped in the statistical memoizer \texttt{mem\&em},
with parameters given by the semicontext \texttt{theta} (see Setion
\ref{sec:thompson-framework}).  \texttt{theta} is given a prior distribution,
and as more actions \texttt{a} are probed, inference is performed on
\texttt{theta}, and subsequent evaluations of the emulator \texttt{r\_emulate}
are affected accordingly.  The action \texttt{a} chosen at each step is
determined by the procedure \texttt{argmax}; for purposes of Thompson sampling,
\texttt{argmax} (which takes a possibly stochastic procedure as its argument)
should be implemented so that \texttt{(argmax func)} returns an approximation to
$\argmax_{\texttt{a}} \Ebkt{\texttt{(func a)}}$.

\begin{figure}[p]
  \centering
  \includegraphics[height=0.8\textheight]{figs/slide1.pdf}
  \caption{
    Top: Diagrammatic representation of Thompson sampling using the statistical
    memoizer $\mm$.  The emulator $\ftt_\emu$ is used to choose a point
    $\xtt_\rmnext$ to acquire; the acquired data are incorporated into the memo
    table and the parameters $\theta$ are updated according to this new
    information.
    Middle: Mathematical specification of the statistical emulator $\ftt_\emu$.
    Bottom: Depiction of the state of the emulator after zero, one, and two
    probes have been taken.  Here the true value function $V$ is drawn in blue;
    the believed distribution on $V$ is drawn in red.  Between probes,
    $\ftt_\emu$ can be \texttt{sample}d but not \texttt{predict}ed, as its state
    (see Section \ref{sec:gps-in-pps}) must not be changed.  The state (i.e.,
    the memo table) must only be changed when $\ftt_\probe$ is called.
  }
  \label{fig:slide1}
\end{figure}
\end{comment}
\myparagraph{A Mathematical Specification}\label{sec:math-spec}
Our mathematical specification assert the following properties:
\begin{itemize}
  \item The regression function has a Gaussian process prior.
  \item The actions $a_1,a_2,\ldots \in \Acal$ are chosen by a Metropolis-like search
    strategy with Gaussian drift proposals.
  \item The hyperparameters of the Gaussian process are inferred using
    Metropolis--Hastings sampling after each action.
\end{itemize}

In this version of Thompson sampling, the contexts $\ttheta$ are Gaussian
processes over the action space $\Acal = [-20, 20] \subseteq \R$.  That is,
\[ V \sim \GP(\mu, K), \]
where the mean $\mu$ is a computable function $\Acal \to \R$ and the covariance
$K$ is a computable (symmetric, positive-semidefinite) function $\Acal \times
\Acal \to \R$.  This represents a Gaussian process $\br{R_a}_{a \in \Acal}$,
where $R_a$ represents the reward for action $a$.  Computationally, we represent
a context as a data structure
\[ \ttheta = (\theta, \abf_\past, \rbf_\past) = (\mu_\prior, K_\prior, \eta, \abf_\past, \rbf_\past), \]
where $\mu_\prior$ is a procedure to be used as the prior mean function. w.l.o.g. we set $\mu_\prior \equiv 0$.
$K_\prior$ is a procedure to be used as the prior covariance function, parameterized by 
$\eta$.

The posterior mean and covariance for such a context $\ttheta$ are gotten by the
usual conditioning formulas (assuming, for ease of exposition as above, that the
prior mean is zero):\footnote{
  Here, for vectors $\abf = \pn{a_i}_{i=1}^{n}$ and $\abf' =
  \pn{a'_i}_{i=1}^{n'}$, $\mu(\abf)$ denotes the vector
  $\pn{\mu(a_i)}_{i=1}^{n}$ and $K(\abf,\abf')$ denotes the matrix
  $\begin{bmatrix} K(a_i, a'_j) \end{bmatrix}_{1 \leq i \leq n, 1 \leq j
  \leq n'}$.
}
\begin{align*}
  \mu(\abf)
  &= \mu\pn{\abf \mvert \abf_\past, \rbf_\past} \\
  &= K_\prior(\abf, \abf_\past)
     \,K_\prior(\abf_\past, \abf_\past)^{-1}
     \,\rbf_\past \\
  K(\abf, \abf)
  &= K\pn{\abf, \abf \mvert \abf_\past, \rbf_\past} \\
  &= K_\prior(\abf, \abf)
     - K_\prior(\abf, \abf_\past)
       \,K_\prior(\abf_\past, \abf_\past)^{-1}
       \,K_\prior(\abf_\past, \abf).
\end{align*}
Note that the context space $\Theta$ is not a finite-dimensional parametric
family, since the vectors $\abf_\past$ and $\rbf_\past$ grow as more samples are
taken.  $\Theta$ is, however, representable as a computational
procedure together with parameters and past samples, as we do in the
representation $\ttheta = (\mu_\prior, K_\prior, \eta, \abf_\past, \rbf_\past)$.

We combine the Update and Sample steps of Algorithm \ref{alg:thompson} by
running a Metropolis--Hastings (MH) sampler whose stationary distribution is the
posterior $P\pn{\theta \mvert \abf_\past, \rbf_\past}$.  The functional forms of
$\mu_\prior$ and $K_\prior$ are fixed in our case, so inference is only done
over the parameters $\eta = \br{\sigma,\ell}$; hence we equivalently write
$P\pn{\sigma,\ell \mvert \abf_\past, \rbf_\past}$ for the stationary
distribution.  We make MH proposals to one variable at a time, using the prior
as proposal distribution:
\[
  Q_\proposal\pn{\sigma',\ell \mvert \sigma,\ell} = P(\sigma')
\]
and
\[
  Q_\proposal\pn{\sigma,\ell' \mvert \sigma,\ell} = P(\ell').
\]
The MH acceptance probability for such a proposal is
\[
  P_\accept\pn{\sigma',\ell' \mvert \sigma,\ell}
  =
  \min\br{1,\ \frac{
    Q_\proposal\pn{\sigma,\ell \mvert \sigma',\ell'}
    }{
    Q_\proposal\pn{\sigma',\ell' \mvert \sigma,\ell}
    }
  \cdot
  \frac{
    P\pn{\abf_\past,\rbf_\past \mvert \sigma',\ell'}
    }{
    P\pn{\abf_\past,\rbf_\past \mvert \sigma,\ell}
    }}
\]
Because the priors on $\sigma$ and $\ell$ are uniform in our case, the term
involving $Q_\proposal$ equals $1$ and we have simply
\begin{align*}
  P_\accept\pn{\sigma',\ell' \mvert \sigma,\ell}
  &=
  \min\br{1,\ \frac{
    P\pn{\abf_\past,\rbf_\past \mvert \sigma',\ell'}
    }{
    P\pn{\abf_\past,\rbf_\past \mvert \sigma,\ell}
    }} \\[2mm]
  &=
  \min\bigg\{1,\ \exp\bigg( -\frac12\bigg(
    \rbf_\past^T K_\prior\pn{\abf_\past, \abf_\past \mvert \sigma', \ell'}^{-1} \rbf_\past \\
  & \qquad\qquad\qquad\qquad\qquad -
    \rbf_\past^T K_\prior\pn{\abf_\past, \abf_\past \mvert \sigma, \ell}^{-1} \rbf_\past
  \bigg)\bigg)\bigg\}.
\end{align*}
The proposal and acceptance/rejection process described above define a
transition operator $\tau_\update$ which is iterated a specified number of
times; the resulting state of the MH Markov chain is taken as the sampled
semicontext $\theta$ in Step \ref{itm:thompson-step-sample} of Algorithm
\ref{alg:thompson}.

For Step \ref{itm:thompson-step-search} (Search) of Thompson sampling, we
explore the action space using an MH-like transition operator $\tau_\search$.
As in MH, each iteration of $\tau_\search$ produces a proposal which is either
accepted or rejected, and the state of this Markov chain after a specified
number of steps is the new action $a$.  The Markov chain's initial state is the
most recent action, and the proposal distribution is Gaussian drift:
\[ Q_\proposal\pn{a' \mvert a} \sim \Ncal(a,\,\propstd^2), \]
where the drift width $\propstd$ is specified ahead of time.  The acceptance
probability of such a proposal is
\[ P_\accept\pn{a' \mvert a} = \min\br{1,\ \exp\pn{-E\pn{a' \mvert a}}}, \]
where the energy function $E\pn{\bullet \mvert a}$ is given by a Monte Carlo
estimate of the difference in value from the current action:
\[ E\pn{a' \mvert a} = -\frac1s \pn{\muhat(a') - \muhat(a)} \]
where
\[ \muhat(a) = \frac{1}{N_\avg} \sum_{i=1}^{N_\avg} \widetilde{r}_{i,a} \]
and
\[ \widetilde{r}_{i,a} \sim \Ncal(\mu(a), K(a,a)) \]
and $\br{\widetilde{r}_{i,a}}_{i=1}^{N_\avg}$ are i.i.d.\ for a fixed $a$.
Here the temperature parameter $s \geq 0$ and the population size $N_\avg$ are
specified ahead of time.  Proposals of estimated value higher than that of the current action are
always accepted, while proposals of estimated value lower than that of the
current action are accepted with a probability that decays exponentially
with respect to the difference in value.
The rate of the decay is determined by the temperature parameter $s$,
where high temperature corresponds to generous acceptance probabilities.
For $s=0$, all proposals of lower value are rejected; for $s=\infty$, all
proposals are accepted.
For points $a$ at which the posterior mean $\mu(a)$ is low but the
posterior variance $K(a,a)$ is high, it is possible (especially when
$N_\avg$ is small) to draw a ``wild'' value of $\muhat(a)$, resulting in a
favorable acceptance probability.




 \begin{figure}
  \setlength{\tabcolsep}{1pt} 
 \centering
\begin{tabular}{rl}
\includegraphics[width=0.6\textwidth]{figs/bayesopt/iteration_vs_error.png}&\includegraphics[width=0.4\textwidth]{figs/bayesopt/legend.png}
\put(-126,133){\scriptsize Trimodal ground truth, drift proposal}
\put(-126,125){\scriptsize Trimodal ground truth, uniform proposal}
\put(-126,117){\scriptsize Bimodal ground truth, uniform proposal}
\put(-126,108){\scriptsize Bimodal ground truth, drift proposal}
\put(-126,100){\scriptsize Local Optima, trimodal function}
\put(-126,92){\scriptsize Local Optima, bimodal function}
\put(-119,78){\scriptsize Ground Truth}
\put(-119,66){\scriptsize Posterior samples}
\put(-119,55){\scriptsize Next Probe}
\put(-119,43){\scriptsize Estimated Optimum}
\put(-119,31){\scriptsize Past Probes}
\end{tabular}
\begin{tabular}{llll}\hline
\multicolumn{1}{|l|}{\raisebox{-0.5\height}{\includegraphics[width=0.3\textwidth]{figs/bayesopt/opt_sequence_uniform_trimodal_7.png}}}&
\multicolumn{1}{l|}{\raisebox{-0.5\height}{\includegraphics[width=0.3\textwidth]{figs/bayesopt/opt_sequence_uniform_trimodal_8.png}}}&
\multicolumn{1}{l|}{\raisebox{-0.5\height}{\includegraphics[width=0.3\textwidth]{figs/bayesopt/opt_sequence_uniform_trimodal_9.png}}} \\ \hline
\multicolumn{1}{|l|}{\raisebox{-0.5\height}{\includegraphics[width=0.3\textwidth]{figs/bayesopt/opt_sequence_uniform_trimodal_10.png}}}&
\multicolumn{1}{l|}{\raisebox{-0.5\height}{\includegraphics[width=0.3\textwidth]{figs/bayesopt/opt_sequence_uniform_trimodal_11.png}}}&
\multicolumn{1}{l|}{\raisebox{-0.5\height}{\includegraphics[width=0.3\textwidth]{figs/bayesopt/opt_sequence_uniform_trimodal_12.png}}} \\ \hline
\end{tabular}
\put(-360,56){\footnotesize$i = 7$}
\put(-238,56){\footnotesize$i = 8$}
\put(-117,56){\footnotesize$i = 9$}
\put(-360,-8){\footnotesize$i = 10$}
\put(-238,-8){\footnotesize$i = 11$}
\put(-117,-8){\footnotesize$i = 12$}
\caption{Top: the estimated optimum over time. Blue and Red represent optimization with uniform and Gaussian drift proposals. Black lines indicate the local optima of the true functions. Bottom: a sequence of actions. Depicted are iterations 7-12 with uniform proposals.}
\end{figure}
Indeed, taking an action $a$ with low estimated value but high uncertainty
serves the useful function of improving the accuracy of the estimated value
function at points near $a$ (see Figure \ref{fig:slide2}).\footnote{
  At least, this is true when we use a smoothing prior covariance function such
  as the squared exponential.
}$^,$\footnote{
  For this reason, we consider the sensitivity of $\muhat$ to uncertainty to be
  a desirable property; indeed, this is why we use $\muhat$ rather than the
  exact posterior mean $\mu$.
}
We see a comlete probabilistic program with \gpmem\ implementing Bayesian optimization
with Thompson Sampling below (Listing \ref{alg:bayesopt}).
 \input{code/code_venBayesOpt.tex}
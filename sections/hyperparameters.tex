We can apply \gpmem\ for regression in a hierarchical Bayesian setting
(Fig. \ref{fig:neal_tutorial}).  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\input{figs/neal_tutorial.tex}
\captionsetup{aboveskip=-7pt}
\caption{\footnotesize Regression with outliers and hierarchical prior
structure.}
\label{fig:neal_tutorial}
\end{figure}
Here, we show hyper-parameter inference for regression using an example
taken from a paper on the treatment of outliers with hierarchical
Bayesian hyper-priors for \ac{GP}s~\citep{neal1997monte}:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
f_\text{true}(x) =  0.3 + 0.4 x + 0.5 \sin(2.7x) + \frac{1.1}{(1+ x^2)} + \eta
\;\;\; with\;\;\eta \sim \mathcal{N}(0,\sigmanoise). \label{eq:f_true}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We emphasize that \gpmem\ can be used with externally collected
data sets whose generating process may not be available at all,
by coding the ``true function'' argument to \gpmem\ as a lookup
in a fixed data set (Fig. \ref{fig:neal_tutorial} (b)).
That data was synthesized offline by applying the $f_\text{true}$ of (\ref{eq:f_true})
to a fixed set of inputs, with outliers given by setting $\sigmanoise = 1$ in 5\% of
the cases (the remaining 95\% ``inliers'' have $\sigmanoise = 0.1$).

We set $\Krv = k^{\text{se}+\text{wn}}$ and parameterize it with $\bm{\theta}=\{sf,\ell,\sigma\}$.
For these hyper-parameters, Neal's work suggests a hierarchical system for
hyper-parameterization.
Here, we draw hyper-parameters from $\Gamma$ distributions:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\label{eq:hyper-ell}
\ell \sim \Gamma(\alpha_1,\beta_1),\;\sigma \sim \Gamma(\alpha_2,\beta_2)
\end{equation} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
and in turn sample the $\alpha$ and $\beta$ from $\Gamma$ distributions as well:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\label{eq:hyper-alpha}
\alpha_1 \sim \Gamma(\alpha^1_{\alpha},\beta^1_{ \alpha } ),\;
\beta_1 \sim \Gamma(\alpha^1_{\beta},\beta^1_{ \beta } ),\;
\alpha_2 \sim \Gamma(\alpha^2_{\alpha},\beta^2_{\alpha}),\;
\beta_2 \sim \Gamma(\alpha^2_{\beta},\beta^2_{\beta}).
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We model this in Venture as illustrated in Fig. \ref{fig:neal_tutorial} (c),
using the built-in \ac{SP} $\texttt{gamma}$. 

In Fig. \ref{fig:neal_tutorial}, panel (d), we see that $k^{\text{se}+\text{wn}}$
is defined as a composite covariance function. It is the sum ($\texttt{add\_funcs}$) of
a squared exponential kernel ($\texttt{make\_squaredexp}$) and a white noise
($\kwn$, Appendix A)
kernel which is implemented with $\texttt{make\_whitenoise}$.\footnote{
Neal \citeyearpar{neal1997monte} uses the sum of an SE
plus a constant kernel. For illustration, we use a WN kernel
instead.}
We then initialize \gpmem, giving it $\texttt{composite\_covariance}$ and the data
look-up function $\texttt{f\_look\_up}$. 
We sample from the prior $\ystar$ with random parameters $\texttt{sf,l}$ and $\texttt{sigma}$ and 
without any observations available (Fig. \ref{fig:neal_tutorial}, panel (e)).
We depict those samples on the right (red), alongside the true function that generated the data (blue) and
the data points we have available in the data set (black).

We can incorporate observations using both \texttt{observe} and \texttt{predict} (Fig. \ref{fig:neal_tutorial} (f)).
When we subsequently sample $\yprime$ from the emulator with
$\mathcal{N}(\mupost,\Kpost)$, we can see that the \ac{GP} posterior incorporates knowledge 
about the $\texttt{data}$. Yet, the hyper-parameters $\texttt{sf,l}$ and $\texttt{sigma}$ are still
random, so the emulator does not capture the true underlying dynamics
($f_\text{true}$) of the \texttt{data} correctly. 

Next, we demonstrate how we can capture these underlying dynamics within only
100  nested \ac{MH} steps on the hyper-parameters to get a good approximation
for their posterior $\yprime$ (Fig. \ref{fig:neal_tutorial} (g)).
We say nested because we first take two sweeps in the scope
$\texttt{hyperhyper}$ which characterizes (\ref{eq:hyper-alpha}) and then one
sweep on the scope $\texttt{hyper}$ which characterizes (\ref{eq:hyper-ell}).
This is repeated 100 times using $\texttt{repeat( 100, do(}\cdots\;$.
Note that Neal devises an additional noise model and performs a large number of Hybrid-Monte Carlo and Gibbs steps to achieve this, whereas inference in Venture with \gpmem\ is merely one line of code. 

Finally, we can change our inference strategy altogether. If we decide that instead of
following a Bayesian sampling approach, we would like to perform empirical optimization,
we do this by only changing one line of code, deploying $\texttt{gradient-ascent}$ instead
of $\texttt{mh}$ (Fig. \ref{fig:neal_tutorial} (h)). 


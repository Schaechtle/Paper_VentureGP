To transfer this idea to probabilistic programming, we now introduce a language construct called a
\emph{statistical memoizer}.  Suppose we have a function $\ftt$ which can be evaluated 
but we wish to learn about the behavior of $\ftt$ using as
few evaluations as possible.  The statistical memoizer, which here we give the
name \gpmem, was motivated by this purpose.  It produces two outputs:
\[ \ftt \xrightarrow{\gpmem} (\ftt_\probe, \ftt_\emu). \]
The function $\ftt_\probe$ calls $\ftt$ and stores the output in a memo
table, just as traditional memoization does.  The function $\ftt_\emu$ is
an online statistical emulator which uses the memo table as its training
data.  A fully Bayesian emulator, modelling the true function $\ftt$ as a
random function $f \sim P(f)$, would satisfy
\[
\texttt{(}\ftt_\emu\ \xtt_1\ \ldots\ \xtt_k\texttt{)}
\sim
P\pn{
  f(\xtt_1), \ldots, f(\xtt_k)
  \mvert
  \text{$f(\xtt) = \texttt{(f x)}$ for each $\xtt$ in memo table}
}.
\]
Different implementations of the statistical memoizer can have
different prior distributions $P(f)$; in this paper, we deploy a Gaussian process
prior (implemented as \texttt{gpmem} below).  Note that we require the ability
to sample $\ftt_\emu$ jointly at multiple inputs because the values of
$f(\xtt_1),\ldots,f(\xtt_k)$ will in general be dependent.
To illustrate the linguistic power of \gpmem\, consider a simple model program
that uses \gpmem\, procedure abstraction, and a loop:

\begin{minipage}{\textwidth}
\small
\begin{lstlisting}[frame=single,escapechar=\#,numbers=left,numbersep=5pt,numberstyle=\tiny\color{gray}]
define choose_next_point = proc(em) { gridsearch_argmax( em) }

 // Here stats( em) is the memo table {(x_i, y_i)}.
 // Take the (x,y) pair with the largest y.
define extract_answer = proc(em) { first(max( stats( em, second)))}
 
 // Hyper-parameter 
assume theta =  tag( quote( params), 0, gamma(1,1))
assume (f_probe f_emu) =  gpmem(f, make_squaredexp(1.0, theta))

for t...T:
  f_probe( choose_next_point( f_emu)) #\label{ln:mem&em-penultimate}#
extract_answer( f_emu)    
\end{lstlisting}
\end{minipage}


An equivalent model in typical statistics notation, written in a way that
attempts to be as linguistically faithful as possible, is
\begin{align*}
  \s{0}f &\sim P(f) \\
  \s{t}\xtt &= \argmax_x \s{t}f(x) \\
  \s{t+1}f &\sim P\pn{\s{t}f \mvert \s{t}f(\s{t}\xtt) = \texttt(\ftt\ \s{t}\xtt\texttt)}.
\end{align*}
The linguistic constructs for abstraction are not present in
statistics notation.  The equation $\s{t}\xtt = \argmax_x \s{t}f(x)$
has to be inlined in statistics notation: while something like
\[ \s{t}\xtt \sim F(\s{t}P) \]
(where $\s{t}P$ is a probability measure, and $F$ here plays the role of
\texttt{choose\_next\_point}) is mathematically coherent, it is not what
statisticians would ordinarily write.  This lack of abstraction then makes
modularity, or the easy digestion of large but finely decomposable models,
more difficult in statistics notation.

Adding a single line to the program, such as
    \begin{lstlisting}
    infer mh( quote( params), one, 50)
    \end{lstlisting}
after line \ref{ln:mem&em-penultimate} to infer the parameters of the
emulator, would require a significant refactoring and elaboration of the statistics
notation.  One basic reason is that probabilistic programs are written
procedurally, whereas statistical notation is declarative; reasoning
declaratively about the dynamics of a fundamentally procedural inference
algorithm is often unwieldy due to the absence of programming constructs
such as loops and mutable state.


Memoization is the practice of storing previously computed values of a function so that future calls with the same inputs can be evaluated by lookup rather than re-computation.
To transfer this idea to probabilistic programming, we now introduce a language construct called a
\emph{statistical memoizer}.  Suppose we have a function $\ftt$ which can be evaluated 
but we wish to learn about the behavior of $\ftt$ using as
few evaluations as possible.  The statistical memoizer, which here we give the
name \gpmem, was motivated by this purpose.  It produces two outputs:
\[ \ftt \xrightarrow{\gpmem} (\ftt_{\text{compute}}, \ftt_\emu). \]
The function $\ftt_{compute}$ calls $\ftt$ and stores the output in a memo
table, just as traditional memoization does.  The function $\ftt_\emu$ is
an online statistical emulator which uses the memo table as its training
data.  A fully Bayesian emulator, modelling the true function $\ftt$ as a
random function $f \sim P(f)$, would satisfy
\[
\texttt{(}\ftt_\emu\ \xtt_1\ \ldots\ \xtt_k\texttt{)}
\sim
P\pn{
  f(\xtt_1), \ldots, f(\xtt_k)
  \mvert
  \text{$f(\xtt) = \texttt{(f x)}$ for each $\xtt$ in memo table}
}.
\]
Different implementations of the statistical memoizer can have
different prior distributions $P(f)$; in this paper, we deploy a \ac{GP} 
prior (implemented as \texttt{gpmem} below).  Note that we require the ability
to sample $\ftt_\emu$ jointly at multiple inputs because the values of
$f(\xtt_1),\ldots,f(\xtt_k)$ will in general be dependent.


\begin{figure}
\input{figs/gpmem_tutorial.tex}
%\put(-245,10){\line(1,0){200}}
\put(-33,77){\color{ForestGreen}\thicklines \vector(0,-1){15}}
\put(-96,6){\color{ForestGreen}\thicklines \vector(0,-1){15}}
\put(-48,-63){\thicklines \vector(0,-1){15}}
\put(-84,-43){\thicklines \vector(0,-1){15}}
\put(-73,-68){\thicklines \vector(0,1){15}}
\put(-460,110){(b)}
\put(-460,45){(c)}
\put(-460,-10){(d)}
\put(-460,-75){(e)}
\put(-460,-130){(f)}
\caption{\small \gpmem\ tutorial. The top shows a schematic of \gpmem.
  \texttt{f\_compute} probes an outside resource.
  This can be expensive (top left).
  Every probe is memoized and improves the emulator. Below the schematic we see the evolution
  of \gpmem's state of believe of the world given certain Venture
  directives. On the right, we depict the true function (blue), samples from the
emulator (red) and incorporated observations (black). We omid code for the
hyper-parameters and \texttt{kse}, it can be found in Appendix D.}
\label{fig:gpmem_tutorial}
\end{figure}

% Panel 1
We explain how \gpmem, the statistical memoizer with \ac{GP}-prior, works using a simple tutorial
(Fig. \ref{fig:gpmem_tutorial}). 
The top panel (Fig. \ref{fig:gpmem_tutorial}, (a)) of this figure sketches the schematic of \gpmem.
$\ftt$ is the external process that we memoize. It can be evaluated using resources that potentially come
from outside of Venture.  
We feed this function into \gpmem\ alongside
a parameterised kernel $k$.  
In this example, we make the qualitative assumption of $f$ being smooth, and define
$k$ to be a squared-exponential covariance function:
\[
k = \kse = \sigma^2 \exp(-\frac{(x-x^\prime)^2}{2\ell^2}).
\]
The hyper-parameters $\thetabf$ for this kernel are sampled from a 
prior distribution which is depicted in the top right box.
Note that we annotate $\bm{\theta}=\{\texttt{sf},\texttt{l}\}$ for subsequent
inference as belonging to (i) the scope "hyper-parameter" and (ii) blocks 0 and 1 respectively.

\gpmem\ implements a memoization table, where all previously
computed function evaluations ($\{\xbf,\ybf\}$) are stored. We also initialize a \ac{GP}-prior that
will serve as our statistical emulator:
\[
P(f_{emu}(x) \mid \xbf,\ybf,\thetabf)\sim
\mathcal{N}\big(\mupost,\Kpost)\big)
\]
where 
\[
P(f_{emu}(x) \mid \mathbf{x},\ybf,\thetabf) = \yprime 
\]
under the traditional \ac{GP} perspective.
All value pairs stored in the memoization table ($\text{memo table} = (\xbf,\ybf)$) are incorporated as observations of
the \ac{GP}.
We simply feed the regression input
into the emulator and output a predictive posterior Gaussian distribution determined by the \ac{GP} and
the memoization table.

% Panel 2
We can either define the function f that serves as as input for \gpmem\
 natively in Venture
(as shown in the Fig. \ref{fig:gpmem_tutorial} (b)) or we interleave Venture with foreign code. 
This can be useful when $\ftt$ is computed with the help of outside resources.
Before making any observations or calls to $\ftt$
we can sample from the prior at the inputs from -20 to 20 using the emulator:
    \begin{lstlisting}
    assume (f_compute, f_emu) = gpmem(f, kse));

    sample f_emu(array(-20, ..., 20));
    \end{lstlisting}
where the second line corresponds to:
\[ 
\ystar \sim \mathcal{N}\Bigg(0,\Kse\bigg(
\bmat{
-20 \\
\cdots \\
20
},
\bmat{
-20 \\
\cdots \\
20
}
\mid \thetabf=\{\sigma,\ell\}
\bigg)
\Bigg).
\]
% Panel 3
In Fig. \ref{fig:gpmem_tutorial} (c), we probe the external function $\ftt$ at point 12.6 and memoize it's result by calling 
   \begin{lstlisting}
    predict f_compute (12.6);
    \end{lstlisting}
When we subsequently sample from the emulator, that is compute the $\yprime$ at the input
$\xprime= \bmat{-20, \cdots, 20}^\top$. We see how the posterior which shifts from uncertainty to near certainty close to the input 12.6.

% Panel 4
We can repeat the process at a different point (probing point -6.4 in Fig.
\ref{fig:gpmem_tutorial} (d)) to see that we gain certainty about another part of the curve. 

% Panel 5
We can add information to $\texttt{f}_\text{emu}$ about presumable value pairs of $\ftt$ without calling $\texttt{f}_\text{compute}$
(Fig. \ref{fig:gpmem_tutorial} (e)).
If a friend tells us the value of $\ftt$ we can call observe to store this information in the incorporated observations for $\texttt{f}_\text{emu}$ only:
    \begin{lstlisting}
    observe f_emu( -3.1) = 2.60;
    \end{lstlisting}
We have this value pair now available for the computation $\yprime$. 
For sampling with the emulator, the effect is the same as calling predict with the $\texttt{f}_\text{compute}$.
However, we can imagine at least one scenario where such as distinction in the treatment of observations 
is beneficial. Let us say we do not only have the real function available but also a domain expert with knowledge 
about this function.
This expert could tell us what the value is at a given input.
Potentially, the value provided by the expert could disagree with the value computed with $\ftt$ for example 
due to different levels of observation noise. 

% Panel 6
Finally, we can update our posterior by inferring the posterior over hyper-parameter values $\thetabf$.
For this we use the defined scopes, which denote a collection of related random choices, such
as all hyper-parameters $\thetabf$. Scopes may be further subdivided into blocks, on 
which block proposals can be made.
In the program above we assign two blocks, 0 and 1 for each hyper-parameter. These tags are supplied to the
inference program (in this case, MH) to specify on which random variables inference
should be done:
    \begin{lstlisting}
    infer mh(scope="hyper-parameters", block=one, steps=50);
    \end{lstlisting}
In this case, we perform one \ac{MH} transition over the scope hyper-parameters
and choose a random block, that is we choose one hyper-parameter at random.
We can also define custom inference actions. Let's define \ac{MH} with Gaussian
drift proposals.
    \begin{lstlisting}
    define drift_kernel = proc(x) { normal(x, 1) };

    define my_markov_chain =
    mh_correct(
	on_subproblem(scope="hyper-parameters", block=2,
	    symmetric_local_proposal(drift_kernel)));
    
    infer my_markov_chain;
    \end{lstlisting}
Note that this inference is not in the Figure. The important part of the above code snippet is \texttt{drift\_kernel}, which is where we say 
that at each step of our Markov chain, we would like to propose a transition by sampling
a new state from a unit normal distribution whose mean is the current state. 
We apply this inference action only for $\texttt{block}$ 2 in the
$\texttt{scope}$ "hyper-parameters".

The newly inferred hyper-parameters allow us now adequately reflect uncertainty
about the curve given all incorporated observations (compare
Fig. \ref{fig:gpmem_tutorial}, bottom panel (f) on  the right with the samples
before inference, one panel above (e)).

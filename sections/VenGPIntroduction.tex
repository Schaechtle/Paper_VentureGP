Probabilistic programming could be revolutionary for machine intelligence due to universal inference engines and the rapid prototyping for novel models~\citep{ghahramani2015probabilistic}.
Probabilistic programming languages aim to provide a formal language to specify probabilistic models in the style of computer programming and can represent any computable probability distribution as a program.
We can write such programs in Venture~\citep{mansinghka2014venture}, the first probabilistic programming language suitable for general purpose use.
Venture comes with scalable performance on hard problems and with a general purpose inference engine.

A family of statistical distributions rarely treated in probabilistic programming languages are \ac{GP}. \ac{GP}s are gaining increasing attention for representing unknown functions by posterior probability distributions in various fields such as machine learning~\citep{rasmussen2006gaussian}, signal processing~\citep{clifton2013gaussian}, computer vision~\citep{kemmler2013one} and bio-medical data analysis~\citep{shepherd2012gaussian}.
When incorporated in probabilistic programming, \ac{GP}s allow us to treat a wide range of problems naturally within the same unified semantic framework.
Examples for hard problems are hierarchical prior construction~\citep{neal1997monte}, systems for inductive learning of symbolic expressions such as the one introduced in the Automated Statistician project~\citep{duvenaud2013structure,lloyd2014automatic} and Bayesian Optimization~\citep{snoek2012practical}.

In the following, we will present \ac{GP} memoization (\gpmem) as a novel probabilistic programming technique that solves such hard problems and provides the necessary semantic framework. \gpmem\ introduces a statistical alternative to standard memoization.  Our contribution is threefold. Firstly, we introduce an efficient implementation of \gpmem\ in form of a self-caching wrapper  that remembers previously computed values. Secondly,we illustrate the statistical emulator that \gpmem\ produces and how it improves with every data point that becomes available. Finally, we show how one can solve hard problems  of state-of-the-art machine learning related to \ac{GP}  using \gpmem.
In the following, we will introduce \ac{GP} memoization. We will start by introducing \gpmem. We then elaborate on its use at hand of the problems mentioned above. To evaluate the performance we use synthetic and real-world data.

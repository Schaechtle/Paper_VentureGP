\acl{GP} (GPs)  are widely used tools in statistics~\citep{barry1986nonparametric}, machine learning~\citep{neal1995bayesian,williams1998bayesian,kuss2005assessing,rasmussen2006gaussian,damianou2013deep}, robotics \citep{ferris2006gaussian}, computer vision~\citep{kemmler2013one}, and scientific computation~\citep{kennedy2001bayesian,schneider2008simulations,kwan2013cosmic}.
% ToDo: get a citation of neal on stats!
They are also central to probabilistic numerics, an emerging effort to develop more computationally efficient numerical procedures, and to Bayesian optimization, a family of meta-optimization techniques that are widely used to tune parameters for deep learning algorithms~\citep{snoek2012practical,gelbart2014bayesian}. 
They have even seen use in artificial intelligence. For example, by searching
over structured kernels generated by a stochastic grammar, the "Automated
Statistician" system can produce symbolic descriptions of time series
~\citep{duvenaud2013structure} that can be translated into natural
language~\citep{lloyd2014automatic}.



This paper shows how to integrate \acsp{GP} into higher-order
probabilistic programming languages and illustrates the utility of this
integration by implementing it for the Venture platform. The key idea is to use
GPs to implement a kind of “statistical” or “generalizing” memoization. The
resulting higher-order procedure, called {\tt gpmem}, takes a kernel function
and a source function and returns a GP-based statistical emulator for the source
function that can be queried at locations where the source function has not yet
been evaluated. When the source function is invoked, new datapoints are
incorporated into the emulator. In principle, the covariance function for the
GP is also allowed to be an arbitrary probabilistic program. This simple packaging covers the full range of uses of the GP described above, including both statistical applications and applications to scientific computation and uncertainty quantification.

This paper illustrates {\tt gpmem} by embedding it in Venture, a general-purpose, higher-order probabilistic programming platform~\citep{mansinghka2014venture}. Venture has several distinctive capabilities that are needed for the applications in this paper. First, it supports a flexible foreign interface for modeling components that supports the efficient rank-1 updates required by standard GP implementations. Second, it provides inference programming constructs that can be used to describe custom inference strategies that combine elements of gradient-based, Monte Carlo, and variational inference techniques. This level of control over inference is key to state-of-the-art applications of GPs. Third, it supports models with stochastic recursion, a priori unbounded support sets, and higher-order procedures; together, these enable the combination of stochastic grammars with a fast GP implementation, needed for structure learning. Fourth, Venture permits nesting of modeling and inference, which is needed for the use of GPs in Bayesian optimization over general objective functions that may in general themselves be derived from modeling and inference.

To the best of our knowledge, this is the first general-purpose integration of
\acsp{GP} into a probabilistic programming language. Unlike software
libraries such as GPy~\citep{gpy2014}, our embedding allows uses of GPs that go beyond classification and regression to include state-of-the art applications in structure learning and meta-optimization.

This paper presents three applications of gpmem: (i) a replication of results by~\citet{neal1997monte} on outlier rejection via hyper-parameter inference; (ii) a fully Bayesian extension to the Automated Statistician project; and (iii) an implementation of Bayesian optimization via Thompson sampling. The first application can in principle be replicated in several other probabilistic languages embedding the proposal that is described in this paper. The remaining two applications rely on distinctive capabilities of Venture: support for fully Bayesian structure learning and language constructs for inference programming. All applications share a single 50-line Python library and require fewer than 20 lines of probabilistic code each.
% ToDo: the 50 lines are bit understated




\ac{GP} are widely used tools in statistics~\citep{barry1986nonparametric}, machine learning~\citep{neal1995bayesian,williams1998bayesian,kuss2005assessing,rasmussen2006gaussian,damianou2013deep}, robotics \citep{ferris2006gaussian}, computer vision~\citep{kemmler2013one}, and scientific computation~\citep{kennedy2001bayesian,schneider2008simulations,kwan2013cosmic}.
% ToDo: get a citation of neal on stats!
They are also central to probabilistic numerics, an emerging effort to develop more computationally efficient numerical procedures, and to Bayesian optimization, a family of meta-optimization techniques that are widely used to tune parameters for deep learning algorithms~\citep{snoek2012practical,gelbart2014bayesian}. They have even seen use in artificial intelligence; for example, they provide the key technology behind a project that produces qualitative natural language descriptions of time series~\citep{duvenaud2013structure,lloyd2014automatic}.

This paper describes Gaussian process memoization, a technique for integrating \ac{GP}s into a probabilistic programming language, and demonstrates its utility by re-implementing and extending state-of-the-art applications of the GP. Memoization, typically implemented by a procedure called {\tt mem()}, is a classic higher-order programming technique in which a procedure is augmented with an input-output cache that is checked each time before the function is invoked. This prevents unnecessary recomputation, potentially saving time at the cost of increased storage requirements. Gaussian process memoization, implemented by the {\tt gpmem()} procedure, generalizes this idea to include a statistical emulator that uses previously computed values as data in a statistical model that can cheaply forecast probable outputs. The covariance function for the Gaussian process is also allowed to be an arbitrary probabilistic program.

This paper presents three applications of gpmem: (i) a replication of results~\citet{neal1997monte} on outlier rejection via hyper-parameter inference; (ii) a fully Bayesian extension to the Automated Statistician project; and (iii) an implementation of Bayesian optimization via Thompson sampling. The first application can in principle be replicated in several other probabilistic languages embedding the proposal that is described in this paper. The remaining two applications rely on distinctive capabilities of Venture: support for fully Bayesian structure learning and language constructs for inference programming. All applications share a single 50-line Python library and require fewer than 20 lines of probabilistic code each.
% ToDo: the 50 lines are bit understated



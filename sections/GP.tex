\ac{GP}s are a Bayesian method for regression. We consider the regression input to be real-valued scalars $x_i$ and the regression output as the value of a function $f$ at $x_i$. The complete training data will be denoted by column vectors $\mathbf{x}$ and $\mathbf{f}$. Unseen test data is denoted with $\mathbf{\hat{x}}$ and $\mathbf{\hat{f}}$.
\ac{GP}s present a non-parametric way to express prior knowledge on the space of all possible functions $f$ modeling
a regression relationship.
Formally, a GP is an infinite-dimensional extension of the multivariate Gaussian distribution.

The collection of random variables $\br{f(x_i)}$ (indexed by $i$) represents the
values of the function $f$ at each location $x_i$.
We drop the index in the following for readability.
We write $f \sim \ac{GP}(m,k)$, where $m$ is the {\em mean function} and $k$ is the {\em covariance function} or {\em kernel}.
That is, $m(x)$ is the prior mean of the random variable $f(x)$, and $k(x,x')$ is the prior covariance of the random variables $f(x)$ and $f(x')$.
Throughout, we write $\Ktheta(\xbf,\xbf^\prime)$ for the prior covariance
matrix determined by a set of hyper-parameters $\thetabf$, $\xbf$ and $\xbf^\prime$, that is, the covariance between the random vectors $\{f(x)\}_{x \in \xbf}$ and $\{f(x')\}_{x' \in \xbf'}$.
We differentiate three different situations, of how  $f$ can be generated with a \ac{GP}:
\begin{enumerate}
\item $\fbf_*$ - we have not seen any training data yet.
    The \ac{GP} samples at any input vector $\xbf_*$ from the prior with
     $\fbf_* \sim \mathcal{N}\big(0,\Ktheta(\xbf_*,\xbf_*)\big)$;
\item $\fbf$ - the observed values for $\fbf:=f(\xbf)$. This is the target value of the value pairs supplied in 
the training data; and
\item $\hat{\fbf}$ - the predictive posterior distribution of test output $\hat\fbf := f(\hat\xbf)$ conditioned on training data $\fbf := f(\xbf)$.
\end{enumerate}We now compute the predictive posterior distribution of test output $\hat\fbf := f(\hat\xbf)$ conditioned on training data $\fbf := f(\xbf)$.  (Here $\xbf$ and $\hat\xbf$ are known constant vectors, and we are conditioning on an observed value of $\fbf$.)  To simplify the calculation, we will assume the prior mean $m$ is identically zero; once the derivation is done, this assumption can be easily relaxed via translation.

The predictive posterior can be computed by first forming the joint density when both training and test data are treated as randomly chosen from the prior, then fixing the value of $\fbf$ to a constant.  To start, let
\[
  \Sigma := \bmat{
    \Ktheta(\xbf, \xbf)     & \Ktheta(\xbf, \hat\xbf)     \\
    \Ktheta(\hat\xbf, \xbf) & \Ktheta(\hat\xbf, \hat\xbf)
  }
  \text{ and }
  \Sigma^{-1} =: \bmat{
    \Mbf_{11} & \Mbf_{12} \\
    \Mbf_{21} & \Mbf_{22}
  }.
\]
We then have
\[
  P(\fbf, \hat\fbf)
  \propto
  \exp\br{
    -\frac12
    \bmat{\fbf^\top & \hat\fbf^\top}
    \bmat{\Mbf_{11} & \Mbf_{12} \\ \Mbf_{21} & \Mbf_{22}}
    \bmat{\fbf \\ \hat\fbf}
  }.
\]
Treating $\fbf$ as a fixed constant, we obtain
\[
  P\pn{\hat\fbf \mvert \fbf}
  \propto
  P(\fbf, \hat\fbf)
  \propto
  \exp\br{
    -\frac12 \hat\fbf^\top \Mbf_{22} \hat\fbf
    - \hbf^\top \hat\fbf
  },
\]
where $\hbf = M_{21} \fbf$ is a constant vector.  Thus $P(\hat\fbf | \fbf)$ is Gaussian,
\begin{equation}\label{eq:pred_posterior}
  P\pn{\hat\fbf \mvert \fbf} \sim \Ncal(\hat\mubf, \hat\Kbf_{\thetabf}),
\end{equation}
with covariance matrix $\hat\Kbf_{\thetabf} = \Mbf_{22}^{-1}$.  To find its mean $\hat\mubf$, we note that $P_{\hat\fbf|\fbf}(\hat\fbf + \hat\mubf)$ is Gaussian with the same covariance as $P(\hat\fbf | \fbf)$, but its exponent has no linear term:
\begin{align*}
  P_{\hat\fbf|\fbf} \pn{\hat\fbf + \hat\mubf \mvert \fbf}
  &\propto
  \exp\br{
    -\frac12 (\hat\fbf + \hat\mubf)^\top \Mbf_{22} (\hat\fbf + \hat\mubf)
    - \hbf^\top (\hat\fbf + \hat\mubf)
  } \\
  &\propto
  \exp\br{
    -\frac12 \hat\fbf^\top \Mbf_{22} \hat\fbf
    - \underbrace{(\hbf + \Mbf_{22} \hat\mubf)^\top}_{\text{must be $0$}} \hat\fbf
  }.
\end{align*}
Thus $\hbf = -\Mbf_{22} \hat\mubf$ and $\hat\mubf = -\Mbf_{22}^{-1} \hbf =
-\Mbf_{22}^{-1} \Mbf_{21} \fbf$.

The partioned inverse equations (\citealp*{barnett1979matrix} following \citealp*{mackay1998introduction}) give
\begin{align*}
  \Mbf_{22} &= \big(\Ktheta(\hat\xbf,\hat\xbf) - \Ktheta(\hat\xbf,\xbf) \Ktheta(\xbf,\xbf)^{-1}
\Ktheta(\xbf,\hat\xbf)\big)^{-1}, \\
  \Mbf_{21} &= -\Mbf_{22} \Ktheta(\hat\xbf,\xbf) \Ktheta(\xbf,\xbf)^{-1}.
\end{align*}
Substituting these in the above gives
\begin{align}
  \hat\Kbf_{\thetabf} &= \Ktheta(\hat\xbf,\hat\xbf) - \Ktheta(\hat\xbf,\xbf)
\Ktheta(\xbf,\xbf)^{-1} \Ktheta(\xbf,\hat\xbf),\label{eq:K_hat} \\
  \hat\mubf &= \Ktheta(\hat\xbf,\xbf) \Ktheta(\xbf,\xbf)^{-1}\fbf.\label{eq:mu_hat}
\end{align}
Together, $\hat\mubf$ and $\hat\Kbf_{\thetabf}$ determine the computation of the predictive posterior
with unseen input data (\ref{eq:pred_posterior}).

Often one assumes the observed regression output is noisily measured, that is,
one only sees the values of $\ybf_\noisy = \mathbf{f}+ \wbf$ where $\wbf$ is
Gaussian white noise with variance $\sigma_\noise^2$. This noise term can be
absorbed into the covariance matrix $\Ktheta(\mathbf{x},\mathbf{x})$ which in the
following, we will write as $\Ktheta$ for readability. The log-likelihood of a \ac{GP} can then be written as:
\begin{equation}
\label{eq:gplogdens}
\log P(\mathbf{f} \mid \xbf) =
-\frac12 \ybf^\top 
\Ktheta^{-1} \ybf
- \frac12\log \abs{\Ktheta}
- \frac{n}{2}\log 2\pi
\end{equation}
where $n$ is the number of data points.
Both log-likelihood and predictive posterior can be computed efficiently using a \ac{SP} in Venture~\citep{mansinghka2014venture}
with an algorithm that resorts to Cholesky factorization\citep[chap. 2]{rasmussen2006gaussian}. 
We write the Cholesky factorization as 
$\mathbf{L} \coloneqq \text{chol}(\Ktheta)$ when
:
\begin{equation}
\Ktheta = LL^\top
\end{equation}
where L is a lower triangular matrix. This allows us to compute the inverse of a covariance matrix as
\begin{equation}
\Ktheta^{-1} = (\mathbf{L}^{-1})^\top (\mathbf{L}^{-1})
\end{equation}
and its determinant as 
\begin{equation}
det(\Ktheta) = det(\mathbf{L})^2
\end{equation}
We compute (\ref{eq:gplogdens}) as
\begin{equation}
\log(P(\mathbf{f}\mid \mathbf{x})\coloneqq - \frac{1}{2} \mathbf{f}^\top \bm{\alpha} - \sum_i \log \mathbf{L}_{ii} - \frac{n}{2} \log 2 \pi
\end{equation}
where 
\begin{equation}
\label{eq:chol_L}
\mathbf{L} \coloneqq \text{chol}(\Ktheta)
\end{equation}
and 
\begin{equation}
\label{eq:alpha}
\bm{\alpha} \coloneqq  \mathbf{L}^\top \backslash(\mathbf{L} \backslash \mathbf{f}). 
\end{equation}
%This results in a computational complexity of $\mathcal{O}(n^3)$ in the number of data points for
%sampling with a complexity of $n^3/6$ for (\ref{eq:chol_L}) an $n^2/2$ for (\ref{eq:alpha}). 
This results in a computational complexity for sampling in the number of data points of $O(n^3/6)$ for (\ref{eq:chol_L}) an $O(n^2/2)$ for (\ref{eq:alpha}). 

Above, we defined the \ac{GP} prior as $\fbf_* \sim \mathcal{N}\big(0,\Ktheta(\xbf_*,\xbf_*)\big)$.
We see that this prior is fully determined by its covariance function.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Covariance Functions}
The covariance function (or kernel) of a \ac{GP} governs high-level properties of the observed data such as smoothness or linearity. A linear covariance can be written as:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}\label{eq:LIN1}
    \text{LIN} =   \sigma_1^2(x x^\prime).
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can also express periodicity:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}\label{eq:PER1}
    \text{PER} =  \sigma_2^2 \exp \bigg( \frac{2 \sin^2 ( \pi (x - x^\prime)/p}{\ell^2} \bigg). 
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
By changing these properties we get completely different prior behavior for sampling $\fbf_*$ from a
\ac{GP} with a linear kernel
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
\fbf_* \sim \mathcal{N}\big(0,\text{LIN}(\xbf,\xbf)\big)
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
as compared to sampling from the prior predictive with a periodic kernel (as depicted in 
Fig. \ref{fig:composition_tutorial} (c) and (d))
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
\fbf_* \sim \mathcal{N}\big(0,\text{PER}(\xbf,\xbf)\big).
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\input{figs/composition/composition.tex}
\caption{We depict kernel composition. 
(a) shows raw data (black) generated with a sine function with linearly growing amplitude (blue).
This data is used for all the plots (c-h). 
(b) shows the linear and the periodic base kernel as well as a composition of both. 
The multiplication of the two kernels indicates local interaction. The local interaction we account for in this case is the growing amplitude (a). For each column (c-h) $\bm{\theta}$ is different.(c-e) show samples from the prior
prior predictive $\fbf_*$ where random parameters are used, that is, we sample before any data points are observed.
(f-h) show samples from the predictive posterior $\hat\fbf$, after the data has been observed.}
\label{fig:composition_tutorial}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
These high-level properties are compositional via addition and multiplication of different covariance functions. 
That means that we can also combine these properties.
By using multiplication of kernels we can model a local interaction of two components, for example 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}\label{eq:LINxPER}
    \text{LIN} \times \text{PER} =  \sigma_1^2(x x^\prime)\, \sigma_2^2 \exp \bigg( \frac{2 \sin^2 ( \pi (x - x^\prime)/p}{\ell^2} \bigg) 
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This results in a combination of the higher level properties of linearity and  periodicity.
In Fig \ref{fig:composition_tutorial} (e) we depict samples for $\fbf_*$ that are periodic
with linearly increasing amplitude.
We consider this a local interaction because the actual interaction depends on the similarity
of two data points.
An addition of covariance functions models a global interaction, that is an interaction of two high-level components that is qualitatively not dependent on the input space. An example for this a periodic function with a linear
trend.

Covariance functions come with  free parameters that we call hyper-parameters which we will
refer to as $\bm{\theta}$.
For each kernel type, each $\bm{\theta}$ is different, that is, in (\ref{eq:LIN1}) we have $\thetabf=\{\sigma_1\}$,
in (\ref{eq:PER1}) we have $\bm{\theta}=\{\sigma_2,p,\ell\}$ and in 
(\ref{eq:LINxPER}) we have $\bm{\theta}=\{\sigma_1,\sigma_2,p,\ell\}$.
Adjusting these hyper-parameters changes lower level qualitative attributes such as length
scales ($\ell$) while preserving the higher level qualitative properties of the distribution
such as linearity.
We write the parameterized covariance function as $\mathbf{K}_{\bm{\theta}}(\xbf,\xbf)$ and the
covariance matrix determined by a parameterized covariance function as $\mathbf{K}_{\bm{\theta}}$.

When we observe the data, that is we condition on the input-output pairs $\{\xbf,\fbf\}$ we can sample from the 
predictive posterior $P(\hat\fbf \mid \fbf)$ given $\mathbf{K}_{\bm{\theta}}$ and respectively $\bm{\hat{\mu}}$ and $\hat{\mathbf{K}}_{\bm{\theta}}$.
If we choose suitable parameters, for example by performing inference, we can capture the underlying dynamics of the data well (see Fig. \ref{fig:composition_tutorial} (f-h)) while sampling $\hat{\fbf}$.
Note that goodness of fit is not only limited to the parameters. A too simple qualitative structure
implies unsuitable behaviour, as for example in (Fig. \ref{fig:composition_tutorial} (g)) where additional 
recurring spikes are introduced to account for the changing amplitude of the true function that 
generated the data.







Let the data be pairs of real-valued scalars $\{(x_i,y_i)\}_{i=1}^n$
(complete data will be denoted by column vectors $\mathbf{x}$, $\mathbf{y}$).
\ac{GP}s present a non-parametric way to express prior knowledge on the space of possible functions $f$ modeling
a regression relationship.
Formally, a GP is an infinite-dimensional extension of the multivariate Gaussian distribution.
Often one assumes the values $\ybf$ are noisily measured, that is, one only sees the values of $\ybf_\noisy = \ybf + \wbf$ where $\wbf$ is Gaussian white noise with variance $\sigma_\noise^2$.
In that case, the log-likelihood of a \ac{GP} is
\begin{equation}
\log p\pn{\ybf_\noisy \mvert \xbf} =
-\frac12 \ybf^\top (\bm{\Sigma}
+ \sigma_\noise^2 \Ibf)^{-1} \ybf
- \frac12\log \abs{\bm{\Sigma} + \sigma_\noise^2 \Ibf}
- \frac{n}{2}\log 2\pi
\end{equation}
where $n$ is the number of data points.
Both log-likelihood and predictive posterior can be computed efficiently in a Venture SP with an algorithm that resorts to Cholesky factorization\citep[chap. 2]{rasmussen2006gaussian} resulting in a computational complexity of $\mathcal{O}(n^3)$ in the number of data points.



The covariance function (or kernel) of a \ac{GP} governs high-level properties of the observed data such as linearity, periodicity and smoothness.
It comes with few free parameters that we call hyper-parameters.
Adjusting these results in minor changes, for example with regards to when two data points are treated similar.
More drastically different covariance functions are achieved by changing the structure of the covariance function itself.
%A different type could be a linear covariance function:
%\begin{equation}
% k(x,x^\prime) = \sigma^2 (x-\ell) (x^\prime-\ell). 
%\end{equation}
Note that covariance function structures are compositional: adding or multiplying two valid covariance functions results in another valid covariance function. 







Venture includes the primitive \texttt{make\_gp}, which takes as arguments a
unary function \texttt{mean} and a binary (symmetric, positive-semidefinite)
function \texttt{cov} and produces a function $\gtt$ distributed as a Gaussian
process with the supplied mean and covariance.  For example, a function $\gtt
\sim \GP(0,\,\SE)$, where $\SE$ is a squared-exponential covariance
\[ \SE(x, x') = \sigma^2 \exp\pn{\frac{(x-x')^2}{2\ell}} \]
with $\sigma=1$ and $\ell=1$, can be instantiated as follows:
\begin{lstlisting}[language=Venture]
assume zero = make_const_func( 0.0)
assume se = make_squaredexp( 1.0, 1.0)
assume g  = make_gp( zero, se)
\end{lstlisting}
There are two ways two view $\gtt$ as a ``random function.'' In the first view,
the \texttt{assume} directive that instantiates $\gtt$ does not use any
randomness---only the subsequent calls to $\gtt$ do---and coherence constraints
are upheld by the interpreter by keeping track of which evaluations of $\gtt$
exist in the current execution trace.  Namely, if the current trace contains evaluations
of $\gtt$ at the points $x_1,\ldots,x_N$ with return values $y_1,\ldots,y_N$,
then the next evaluation of $\gtt$ (say, jointly at the points $x_{N+1}, \ldots,
x_{N+n}$) will be distributed according to the joint conditional distribution
\[
  P\pn{\big.
    \texttt(\gtt\ x_{N+1}\texttt), \ldots, \texttt(\gtt\ x_{N+n}\texttt)
    \mvert
    \texttt(\gtt\ x_i\texttt) = y_i \text{ for $i=1,\ldots,N$}}.
\]
In the second view, $\gtt$ is a randomly chosen deterministic function, chosen
from the space of all deterministic real-valued functions; in this view, the
\texttt{assume} directive contains \emph{all} the randomness, and subsequent
invocations of $\gtt$ are deterministic.  The first view is procedural and is
faithful to the computation that occurs behind the scenes in Venture.  The
second view is declarative and is faithful to notations like ``$g \sim P(g)$''
which are often used in mathematical treatments.  Because a model program could
make arbitrarily many calls to $\gtt$, and the joint distribution on the return
values of the calls could have arbitrarily high entropy, it is not
computationally possible in finite time to choose the entire function $\gtt$ all
at once as in the second view.  Thus, it stands to reason that any
computationally implementable notion of ``nonparametric random functions'' must
involve incremental random choices in one way or another, and Gaussian processes
in Venture are no exception.
\begin{comment}
Behind the scenes, Venture attaches a state $D = (\xbf_\past, \ybf_\past)$ to
each procedure created by \texttt{make\_gp}, where $\xbf_\past$ is a vector of
all past inputs to $\gtt$ in the current trace, and $\ybf_\past$ is the
corresponding vector of outputs.  (The state $D$ is updated each time an
invocation of $\gtt$ is added to or removed from the current trace.)  Thus, the
incremental random choices made by a GP $\gtt$ in Venture are simply samples
from the conditional distribution $P\pn{\gtt \mvert D}$.  That is, if $\gtt$ was
initialized as \texttt{make\_gp mean cov)} and has accrued state $D =
(\xbf_\past, \ybf_\past)$, then
\[
\texttt{(g }\xbf\texttt{)}
\sim
P\pn{
  \texttt{(g}_0\ \xbf\texttt{)}
  \mvert
  \texttt{(g}_0\ \xbf_\past\texttt{)} = \ybf_\past
}, \qquad\text{where $\gtt_0 \sim \texttt{make\_gp mean cov)}$}.
\]

 


Statefully represented random functions highlight the difference between
\texttt{predict} and \texttt{sample} (see Section \ref{sec:interactivity}).  To
illustrate, consider the following program:

\begin{mdframed}
\begin{lstlisting}[language=Venture,escapechar=\#]
assume zero = make_const_func( 0.0)
assume se =  make_squaredexp( 1.0, 1.0)
assume g = make_gp( zero, se)

call_back( draw_gp_curves( g( ...)))
#\includegraphics[width=10cm]{figs/interactive_gp_prior.png}#
sample( g( array(1 2 3)))
\\ [-1.04331626 -0.72016875 -0.44525212]
call_back( draw_gp_curves( g( ...)))
#\includegraphics[width=10cm]{figs/interactive_gp_after_sample.png}#
predict( g( array(1 2 3)))
\\ [ 0.55736593  0.09784888  0.56553757]
call_back( draw_gp_curves( g( ...)))
#\includegraphics[width=10cm]{figs/interactive_gp_after_predict.png}#
\end{lstlisting}
\end{mdframed}
(The callback \texttt{draw\_gp\_curves} draws a cloud of curves sampled from
$\gtt$.)  The \texttt{sample} directive does not affect the state of $\gtt$, so
the distribution of $\gtt$ after the \texttt{sample} is the same as the prior.
The \texttt{predict}, however, does affect the state; the distribution of $\gtt$
after the \texttt{predict} directive is conditioned on the values returned by
$\gtt$ inside the directive.  This explains why, in the final plot, all curves
go through the points returned by the previous \texttt{predict} directive, and
do not go through the points returned by the \texttt{sample} directive (which
are marked with black dots for reference).


\end{comment}
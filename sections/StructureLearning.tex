Inductive learning of symbolic expressions for continuous-valued time series
data is a hard task which has recently been tackled using a greedy search over 
the approximate posterior of the possible kernel compositions for
\ac{GP}s~\citep{duvenaud2013structure,lloyd2014automatic}\footnote{\url{http://www.automaticstatistician.com/}}.

With \gpmem\ we can provide a fully Bayesian treatment of this, previously unavaible,
using a stochastic grammar  (see Fig. \ref{fig:schema}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/schematic.tex}
\caption{\small(a) Graphical schematic of Bayesian GP structure learning. A set of
base kernels with priors on there hyper-parameters serve as hypothesis space.
Along with a distribution of the possible size of the composite kernel, this is
supplied as input to the stochastic grammar. The stochastic grammar generates
first a subset of the base kernels and then a composite structure which serves as input for
\gpmem.  We observe value pairs of unstructured time series data on the bottom of the schematic. We can also generate predictions for future points in time. (b) An example of composite kernel structure. We use addition and multiplication to combine base kernels.
Base kernels and compositional kernels are shown in (c) alongside the natural language interpretation of the structure.}\label{fig:schema}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This allows us to read an unstructured time series and automatically output a high-level,
qualitative description of it. The stochastic grammar takes two inputs:
\begin{enumerate}
    \item a set of base kernels 
    $\{\Kbf_{\bm{\theta}^1}^1,\cdots,\Kbf_{\bm{\theta}^m}^m\}$
    of size $m$, including their corresponding parametrization
    $\thetabf^*=\{\thetabf^1,\cdots,\thetabf^m\}$ (Fig. \ref{fig:schema} (a) and (b)); and
    \item a prior distribution on the number $n \leq m$ that determines how many of the base kernels are used for
    the composition. This can be uniform over the size, that is each possible
    $n$ is equally likely, or over the compositions, that is larger $n$ are more
    likely since they can yield a larger number of structures.
\end{enumerate}
Internally, the stochastic grammar first samples the number of base kernels that it is
going to combine into a compositional kernel,
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
n \sim P(n),
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
using the supplied prior. We use this number to sample a random subset S of size $n$ of
the set of supplied base kernels. We write
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
S = \{\Kbf_{\bm{\theta}^i}^i,\cdots,\Kbf_{\bm{\theta}^n}^n\}
\sim P(S = \{\Kbf_{\bm{\theta}^i}^i,\cdots,\Kbf_{\bm{\theta}^n}^n\} \mid n) 
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
with
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
P(S = \{\Kbf_{\bm{\theta}^i}^i,\cdots,\Kbf_{\bm{\theta}^n}^n\}\mid n) = \frac{n!}{ \mid S = \{\Kbf_{\bm{\theta}^i}^i,\cdots,\Kbf_{\bm{\theta}^n}^n\}\mid !}.
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The only building block that we are now missing is how to combine the sampled
base kernels into a compositional covariance function (see Fig. \ref{fig:schema}
(b)). For each interaction $i$, we
have to infer whether the data supports a local interaction or a global interaction,
chosing between one out of two algebraic operators
$\bm{\Omega}_i=\{+,\times\}$. The probability for all such decisions is given by a binomial distribution: 
\begin{equation}
P(\bm{\Omega} \mid S,n)= {n \choose r}  p_{+\times}^r (1 - p_{+\times})^{n-r}.
\end{equation}
We can write the marginal probability of a composite structure as
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
P(\mathbf{K}_{\bm{\theta}} \mid \bm{\Omega},S,n) = P(\bm{\Omega} \mid S,n)\times P(S \mid n) \times P(n),
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
with $\bm{\theta}\subseteq \bm{\theta}^*$. This  composite kernel is fed into \gpmem. 
The emulator generated by \gpmem\ observes unstructured time series data. We
show an implementation of our approach with \gpmem\ in Listing
\ref{alg:structureVent}. 








Many equivalent covariance structures can be sampled due to covariance function algebra
and equivalent representations with different parameterization~\citep{lloyd2014automatic}.
To inspect the posterior of these equivalent structures we convert each kernel expression
into a sum of products and subsequently simplify. All base kernels can be found in Appendix A,
rules for this simplification can be found in appendix B. The code for learning of kernel structure 
is as follows:

\input{code/code_venstructure.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%     Mauna result      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We defined a simple space of covariance structures in a way that allows us to produce results coherent with 
work presented in Automatic Statistician. We will illustrate our results with two data sets.

\myparagraph{Mauna Loa  CO$_2$ data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/structure_posterior.tex}
\caption{\small Structure Learning. Starting with raw data (a), we fit a \ac{GP}
(b) and compute the posterior distribution on structures (c). We take a sample
of the peak of this distribution ($\text{LIN}+\text{PER}+\text{SE}+\text{WN}$)
including its parameters and write it in functional form (d). We depict the
human readable interpretation (e). We used (d) to plot (b).}\label{fig:posterior}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We illustrate results in Fig \ref{fig:posterior}. In Fig \ref{fig:posterior} (a) we depict the raw data. 
We see mean centered CO$_2$ measurements of the Mauna Loa Observatory, an atmospheric
baseline station on Mauna Loa, on the island of Hawaii. 
A description of the data set  can be found in  \citealp[][chapter 5]{rasmussen2006gaussian}.  
We use those raw data to compute a posterior on structure, parameters and \ac{GP}
samples.
The latter are shown in  Fig \ref{fig:posterior} (b)
where we zoom in to show how the posterior captures the error bars
adequately.
This posterior of the \ac{GP} is generated with a random sample from the parameters
of the peak of the distribution on structure (Fig \ref{fig:posterior} (c)).
The distribution peaks at:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\Kbf_{\thetabf}=\text{LIN} + \text{PER} + \text{SE} + \text{WN}.
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We write this Kernel equation out in Fig \ref{fig:posterior} (d).
This kernel structure has a natural language interpretation that we spell out in
Fig \ref{fig:posterior} (e), explaining that 
the posterior peaks at a kernel structure with four additive components.
Each of which holds globally, that is there are no higher level, qualitative aspects
of the data that vary with the input space. The additive components for this result are as follows:
\begin{itemize}
\item a linearly increasing function or trend; 
\item a periodic function;
\item a smooth function; and
\item white noise.
\end{itemize}
 



Previous work on automated kernel discovery~\citep{duvenaud2013structure} illustrated the Mauna Loa data using an RQ kernel.
We resort to the white noise kernel instead of RQ (similar to \citep{lloyd2014automatic}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%     Airline result   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myparagraph{Airline Data}
The second data set (Fig. \ref{fig:posterior_airline}) we depict results for is  the airline 
data set describing monthly totals of international airline passengers (\citealp{box2011time}, according to \citealp{duvenaud2013structure}). 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/structure_posterior_airline.tex}
\caption{\small Structure Learning. Starting with raw data (a), we fit a \ac{GP}
(b) and compute the posterior distribution on structures (c). We take a sample
of the peak of this distribution ($\text{LIN}+\text{PER} \times \text{SE}+\text{WN}$)
including its parameters and write it in functional form (d). We depict the
human readable interpretation (e). We used (d) to plot (b).}\label{fig:posterior_airline}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We illustrate results for this data set in Fig \ref{fig:posterior_airline}. In Fig \ref{fig:posterior_airline} (a) we depict the raw data. 
Again, the data is mean centered and we use it to 
compute a posterior on structure, parameters and \ac{GP}
samples.
The latter are shown in  Fig \ref{fig:posterior_airline} (b).
This posterior of the \ac{GP} is generated with a random sample from the parameters
of the peak of the distribution on structure (Fig \ref{fig:posterior_airline} (c)).
The distribution peaks at:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\Kbf_{\thetabf}=\text{LIN} +  \text{SE} \times \text{PER}+ \text{WN}.
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We write this Kernel equation out in Fig \ref{fig:posterior_airline} (d).
This kernel structure has a natural language interpretation that we spell out in
Fig \ref{fig:posterior_airline} (e), explaining that 
the posterior peaks at a kernel structure with three additive components.
Additive components hold globally, that is there are no higher level, qualitative aspects
of the data that vary with the input space.
The additive components are as follows: 
\begin{itemize}
\item a linearly increasing function or trend;
\item a approximate periodic function; and
\item  white noise.
\end{itemize}
Both datasets served as illustrations in the Automatic Statistician project.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% Queries for time series %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myparagraph{Querying time series}
With our Bayesian approach to structure learning we can gain valuable insights
into time series data that were previously unavailable.
This is due to our ability to estimate posterior marginal probabilities over the kernel structure.
Over this marginal, we define boolean search operations that allow us to query the data
for the probability of certain structures to hold true globally.
We write
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\label{eq:bool_present}
P(\mathbf{K}) = \frac{1}{N}
\sum\limits_{n=1}^N f(\mathbf{K}^n)\;\;\text{where}\, f(\mathbf{K}^n) = \begin{cases}
  1, & \text{if } \mathbf{K} \underset{global}{\in} \mathbf{K}^n, \\
  0, & \text{otherwise}.
\end{cases} 
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
to ask whether it is true that a global structure $\mathbf{K}$ is present. $N$
is the number of all posterior samples for $\Ktheta$ and $\Kbf^n$ is one such
sample. We omit $\bm{\theta}$ in the following to indicate that the qualitative
type of structure does not depend on a specific parameterization. 
We can now ask simple questions, for example:
\begin{quotation}
Is there white noise in the data?
\end{quotation}
where we set $\mathbf{K} = $WN in (\ref{eq:bool_present}).
We can also formulate more sophisticated search operations using Boolean operators such as AND ($\land$) and OR ($\lor$).
The AND operator is defined as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
P(\mathbf{K}^a \land \mathbf{K}^b)  = \frac{1}{N}
\sum\limits_{n=1}^N f(\mathbf{K}^n)\;\;\text{where}\,f(\mathbf{K}^n) = \begin{cases}
  1, & \text{if } \mathbf{K}^a\, \text{and}\, \mathbf{K}^b  \underset{global}{\in} \mathbf{K}^n, \\
  0, & \text{otherwise}.
\end{cases}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
By estimating $P(\text{LIN} \land \text{WN})$ we can use this operator to ask questions such as 
\begin{quotation}
Is there a Linear component AND a white noise in the data? 
\end{quotation}
Finally, we define the logical OR as
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
P(\mathbf{K}^a \lor \mathbf{K}^b) = P(\mathbf{K}^a) + P(\mathbf{K}^b) - P(\mathbf{K}^a \land \mathbf{K}^b)
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
which allows us to ask questions about structures that are logically connected with OR, such as:
\begin{quotation}
Is there white noise or heteroskedastic noise?
\end{quotation}
by estimating $P(\text{LIN} \times \text{WN}\;\;{\large\lor}\;\; \text{WN})$.
We know that noise can either be heteroskedastic or white,
and we also know due to simple manipulations using kernel algebra
that  $\text{LIN} \times \text{WN}$ and $\text{WN}$ are the only possible ways to construct noise with kernel composition, we see that we can generalize the 
question above to:
\begin{quotation}
Is there noise in the data? 
\end{quotation}
where we write the marginal posterior on qualitative structure for noise:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
P(\mathbf{K}^{\text{noise}}) = P(\text{LIN} \times \text{WN}\;\;{\large\lor}\;\; \text{WN}).
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Note that this allows us to start with general queries and 
subsequently formulate follow up queries that go into more detail.
For example, we could start with a general query, such as:
\begin{quotation}
What is the probability of a trend, a recurring pattern {\bf and} noise in the data?
\end{quotation}
and then follow up with more detailed questions (Fig \ref{fig:query}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/structureQuery.tex}
\caption{{\bf Querying structural motifs in in time series using posterior inference
over kernel structure.} The kernel structure serves as a way to formulate
natural language questions about the data (blue). The initial question of interest
(top) is a fairly
general one: "What is the probability of a trend, a recurring
pattern and noise in the data?" Below the natural language version of this
question, the same question is formulated as an inference problem (black) over the
marginal probability on kernels with Boolean operators AND ($\land$) and OR ($\lor$). 
To gain  a deeper understanding of specific motifs in the time series more specific queries can
be written.
On the right, a query asks whether there is noise in the data (blue) by computing the disjunction of the marginal
of a global white noise kernel and a multiplication between a linear and a white
noise kernel (black). Samples from the predictive prior $\fbf_*$ of such kernels give an
indication of the qualitative aspects that a kernel structure implies (coloured curves below
the marginal). 
If the probability that there is noise in the data is high then it makes sense
to drill even deeper asking more detailed questions. With regards to noise, this
translates to querying whether or not the data supports the hypothesis that there is
heteroskedastic noise or white noise. Queries for motifs of repeating structure
are shown in the middle of the tree, queries related to trends on the left.}\label{fig:query}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This way of querying your data for their statistical implications is in stark contrast to what previous research in automatic kernel construction was able to provide.
We could view our approach as a time series search engine which allows us to test whether or not certain structures can be found
in an available time series.
Another way to view this approach is as a new language to interact with the world.
Real-world observations often come with time-stamps and in form
of continuous valued sensor measurements.  
We provide the toolbox to query such observations in a similar manner as
one would query a knowledge base in a logic programming language.







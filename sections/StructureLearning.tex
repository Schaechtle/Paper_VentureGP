Inductive learning of symbolic expression for continuous-valued time series
data is a hard task which has recently been tackled using a greedy search over 
the approximate posterior of the possible kernel compostions for
\ac{GP}s~\citep{duvenaud2013structure,lloyd2014automatic}\footnote{\url{http://www.automaticstatistician.com/}}.

With \gpmem\ we can provide a fully Bayesian treatment of this, previously unavaible,
using a stochastic grammar  (see Fig. \ref{fig:schema}).
The stochastic grammar samples kernels that are parametrized and then fed into
\gpmem\ (see Fig. \ref{fig:schema}, (a)) which generates an emulator. 
The emulator generated by \gpmem\ observes unstructured time series data. 
The aim here is to find a kernel structure (Fig. \ref{fig:schema} (b)), that is 
compositions of different base kernels (Fig. \ref{fig:schema} (c)).

\begin{figure}
\centering
\input{figs/schematic.tex}
\caption{(a) Graphical description of Bayesian GP structure learning. A stochastic grammar generates samples of kernels that are used as input for \gpmem. Hyper-parameters are sampled accordingly, that is, as needed for a
certain structure. We observe value pairs of unstructured time series data on the bottom of the schematic. We can also generate predictions for future points in time. (b) An example of composite kernel structure. We use addition and multiplication to combine base kernels.
Base kernels and compositional kernels are shown in (c) alongside the natural language interpretation of the structure.}\label{fig:schema}
\end{figure}
The stochastic grammar models a prior on different structural compositions of the covariance function. An input of non-composite kernels (base kernels) is supplied to generate a posterior distributions of composite structure to express local and global aspects of the data.


We approximate the following intractable integrals of the expectation for the prediction:
\begin{equation}
\mathbb{E}[\hat{f} \mid \hat{x},\mathbf{D},\mathbf{K}] =\iint f(\hat{x},\bm{\theta},\mathbf{K})\,P(\bm{\theta} \mid \mathbf{D,\mathbf{K}})\,P(\mathbf{K}|\bm{\Omega},s,n) \; \mathbf{d} \bm{\theta} \mathbf{d} \mathbf{K}.  
\end{equation}

This is done by sampling from the posterior probability distribution of the hyper-parameters and the possible kernel:
\begin{equation}
\hat{f} \approx \frac{1}{T} \sum^T_{t=1} f(\hat{x} | \bm{\theta}^{(t)},\mathbf{K}^{(t)}). 
\end{equation}


In order to provide the sampling of the kernel, we introduce a stochastic process that simulates the grammar for algebraic expressions of covariance function algebra:
\begin{equation}
\mathbf{K}^{(t)} \sim  P(\mathbf{K} \mid \bm{\Omega},s,n)
\end{equation}
Here, we start with the set of given base kernels and draw a random subset.
For this subset of size $n$, we sample a set of possible operators $\bm{\Omega}$ combining base kernels. 
The marginal probability of a composite structure
\begin{equation}
P(\mathbf{K} \mid \bm{\Omega},s,n) = P(\bm{\Omega} \mid s,n)\times P(s \mid n) \times P(n),
\end{equation}
is characterized by the prior $P(n)$ on the number of base kernels used, the probability of a uniformly chosen subset of the set of $n$ possible covariance functions
\begin{equation}
\label{eq:subsets}
P(s \mid n) = \frac{n!}{ \mid s \mid !},
\end{equation}
and the probability of sampling a global or a local structure, which is given by a binomial distribution: 
\begin{equation}
P(\bm{\Omega} \mid s,n)= {n \choose r}  p_{+\times}^r (1 - p_{+\times})^{n-r}.
\end{equation}



Many equivalent covariance structures can be sampled due to covariance function algebra
and equivalent representations with different parameterization~\citep{lloyd2014automatic}.
To inspect the posterior of these equivalent structures we convert each kernel expression
into a sum of products and subsequently simplify. All base kernels can be found in Appendix A,
rules for this simplification can be found in appendix B. The code for learning of kernel structure 
is as follows:

\input{code/code_venstructure.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%     Mauna result      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We defined a simple space of covariance structures in a way that allows us to produce results coherent with 
work presented in Automatic Statistician. We will illustrate our results with two data sets.

\myparagraph{Mauna Loa  CO$_2$ data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/structure_posterior.tex}
\caption{\small Posterior of structure and qualitative, human interpretable reading. We take the raw data (top), compute a posterior distribution on structures (red samples and bar plot).
We take the peak of this distribution ($\text{LIN}+\text{PER}+\text{SE}+\text{WN}$) with the sampled parameters used to generate the samples for the second plot from the top and write it in functional form with parameters. We depict the human readable interpretation of the equation on the bottom.}\label{fig:posterior}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We illustrate results in Fig \ref{fig:posterior}. In Fig \ref{fig:posterior} (a) we depict the raw data. 
We see mean centered CO$_2$ measurements of the Mauna Loa Observatory, an atmospheric
baseline station on Mauna Loa, on the island of Hawaii. 
A description of the data set  can be found in  \citealp[][chapter 5]{rasmussen2006gaussian}.  
We use those raw data to compute a posterior on structure, parameters and \ac{GP}
samples.
The latter are shown in  Fig \ref{fig:posterior} (b)
where we zoom in to show how the posterior captures the error bars
adequately.
This posterior of the \ac{GP} is generated with a random sample from the parameters
of the peak of the distribution on structure (Fig \ref{fig:posterior} (c)).
The distribution peaks at:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\mathbf{K}=\text{LIN} + \text{PER} + \text{SE} + \text{WN}.
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We write this Kernel equation out in Fig \ref{fig:posterior} (d).
This kernel structure has a natural language interpretation that we spell out in
Fig \ref{fig:posterior} (e), explaining that 
the posterior peaks at a kernel structure with four additive components.
Each of which holds globally, that is there are no higher level, qualitative aspects
of the data that vary with the input space. The additive components for this result are as follows:
\begin{itemize}
\item a linearly increasing function or trend; 
\item a periodic function;
\item a smooth function; and
\item white noise.
\end{itemize}
 



Previous work on automated kernel discovery~\citep{duvenaud2013structure} illustrated the Mauna Loa data using an RQ kernel.
We resort to the white noise kernel instead of RQ (similar to \citep{lloyd2014automatic}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%     Airline result   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myparagraph{Airline Data}
The second data set (Fig. \ref{fig:posterior_airline}) we depict results for is  the airline 
data set describing monthly totals of international airline passengers (\citealp{box2011time}, according to \citealp{duvenaud2013structure}). 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/structure_posterior_airline.tex}
\caption{\small Posterior of structure and qualitative, human interpretable reading. We take the raw data (top), compute a posterior distribution on structures (red samples and bar plot).
We take the peak of this distribution ($\text{LIN}+\text{PER} \times \text{SE}+\text{WN}$) with the sampled parameters used to generate the samples for the second plot from the top and write it in functional form with parameters. We depict the human readable interpretation of the equation on the bottom.}\label{fig:posterior_airline}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We illustrate results for this data set in Fig \ref{fig:posterior_airline}. In Fig \ref{fig:posterior_airline} (a) we depict the raw data. 
Again, the data is mean centered and we use it to 
compute a posterior on structure, parameters and \ac{GP}
samples.
The latter are shown in  Fig \ref{fig:posterior_airline} (b).
This posterior of the \ac{GP} is generated with a random sample from the parameters
of the peak of the distribution on structure (Fig \ref{fig:posterior_airline} (c)).
The distribution peaks at:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\mathbf{K}=\text{LIN} +  \text{SE} \times \text{PER}+ \text{WN}.
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We write this Kernel equation out in Fig \ref{fig:posterior_airline} (d).
This kernel structure has a natural language interpretation that we spell out in
Fig \ref{fig:posterior_airline} (e), explaining that 
the posterior peaks at a kernel structure with three additive components.
Additive components hold globally, that is there are no higher level, qualitative aspects
of the data that vary with the input space.
The additive components are as follows: 
\begin{itemize}
\item a linearly increasing function or trend;
\item a approximate periodic function; and
\item  white noise.
\end{itemize}
Both datasets served as illustrations in the Automatic Statistician project.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% Queries for time series %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myparagraph{Querying time series}
With our Bayesian approach to structure learning we can gain valuable insights
into time series data that were previously unavailable.
This is due to our ability to estimate posterior marginal probabilities over the kernel structure.
Over this marginal, we define boolean search operations that allow us to query the data
for the probability of certain structures to hold true globally.
We write
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\label{eq:bool_present}
P(\mathbf{K}) = \frac{1}{N}
\sum\limits_{n=1}^N f(\mathbf{K}_n)\;\;\text{where}\, f(\mathbf{K}_n) = \begin{cases}
  1, & \text{if } \mathbf{K} \underset{global}{\in} \mathbf{K}_n, \\
  0, & \text{otherwise}.
\end{cases} 
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
to ask whether it is true that a global structure $\mathbf{K}$ is present.
We can now ask simple questions, for example:
\begin{quotation}
Is there white noise in the data?
\end{quotation}
where we set $\mathbf{K} = $WN in (\ref{eq:bool_present}).
We can also formulate more sophisticated search operations using Boolean operators such as AND ($\land$) and OR ($\lor$).
The AND operator is defined as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
P(\mathbf{K}_a \land \mathbf{K}_b)  = \frac{1}{N}
\sum\limits_{n=1}^N f(\mathbf{K}_n)\;\;\text{where}\,f(\mathbf{K}_n) = \begin{cases}
  1, & \text{if } \mathbf{K}_a\, \text{and}\, \mathbf{K}_b  \underset{global}{\in} \mathbf{K}_n, \\
  0, & \text{otherwise}.
\end{cases}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
By estimating $P(\text{LIN} \land \text{WN})$ we can use this operator to ask questions such as 
\begin{quotation}
Is there a Linear component AND a white noise in the data? 
\end{quotation}
Finally, we define the logical OR as
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
P(\mathbf{K}_a \lor \mathbf{K}_b) = P(\mathbf{K}_a) + P(\mathbf{K}_b) - P(\mathbf{K}_a \land \mathbf{K}_b)
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
which allows us to ask questions about structures that are logically connected with OR, such as:
\begin{quotation}
Is there white noise or heteroskedastic noise?
\end{quotation}
by estimating $P(\text{LIN} \times \text{WN}\;\;{\large\lor}\;\; \text{WN})$.
We know that noise can either be heteroskedastic or white,
and we also know due to simple manipulations using kernel algebra
that  $\text{LIN} \times \text{WN}$ and $\text{WN}$ are the only possible ways to construct noise with kernel composition, we see that we can generalize the 
question above to:
\begin{quotation}
Is there noise in the data? 
\end{quotation}
where we write the marginal posterior on qualitative structure for noise:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
P(\mathbf{K}_{\text{noise}}) = P(\text{LIN} \times \text{WN}\;\;{\large\lor}\;\; \text{WN}).
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Note that this allows us to start with general queries and 
subsequently formulate follow up queries that go into more detail.
For example, we could start with a general query, such as:
\begin{quotation}
What is the probability of a trend, a recurring pattern {\bf and} noise in the data?
\end{quotation}
and then follow up with more detailed questions (Fig \ref{fig:query}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/structureQuery.tex}
\caption{We can query the data if some logical statements are probable to be true, for example, is it true that there is a trend?}\label{fig:query}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This way of querying your data for their statistical implications is in stark contrast to what previous research in automatic kernel construction was able to provide.
We could view our approach as a time series search engine which allows us to test whether or not certain structures can be found
in an available time series.
Another way to view this approach is as a new language to interact with the world.
Real-world observations often come with time-stamps and in form
of continuous valued sensor measurements.  
We provide the toolbox to query such observations in a similar manner as
one would query a knowledge base in a logic programming language.







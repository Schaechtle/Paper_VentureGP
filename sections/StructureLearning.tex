Inductive learning of symbolic expressions for continuous-valued time series
data is a hard task which has recently been tackled using a greedy search over 
the approximate posterior of the possible kernel compositions for
\ac{GP}s~\citep{duvenaud2013structure,lloyd2014automatic}\footnote{\url{http://www.automaticstatistician.com/}}.

With \gpmem\ we can provide a fully Bayesian treatment of this, previously unavaible,
using a stochastic grammar  (see Fig. \ref{fig:schema}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/schematic.tex}
\caption{\footnotesize (a) Bayesian GP structure learning. A set of
base kernels (BK) with priors on there hyper-parameters serve as hypothesis space
and is supplied as input to the stochastic grammar. The stochastic grammar has
two parts: (i) a sampler that selects a random set $\Sbf$  of primitive kernels from BK
and (ii) a kernel composer that combines the individual base kernels and generates
a composite kernel function
$k_{\thetabf}$. This serves as input for
\gpmem.  We observe value pairs $\xbf,\ybf$ of unstructured time series data on
the bottom of the schematic.
(b) An example of composite kernel structure. Struct($\ktheta$) takes as input a
kernel $\ktheta$ as sampled in (a) and interprets it symbolically and simplifies
it. Rules for simplification can be found in Appendix B. We use addition and multiplication to combine base kernels.
Base kernels and compositional kernels are shown in (c) alongside ther
interpretation with Struct().}\label{fig:schema}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This allows us to read an unstructured time series and automatically output a high-level,
qualitative description of it. The stochastic grammar takes a set of primitive base kernels 
    $\text{BK}=\{k_{\bm{\theta}^1}^1,\cdots,k_{\bm{\theta}^m}^m\}$
    of size $m$, including their corresponding parametrization
    $\thetabf^*=\{\thetabf^1,\cdots,\thetabf^m\}$ (Fig. \ref{fig:schema} (a) and (b))
We depict the input for the
stochastic grammar in Listing \ref{alg:base_kernels}.
\input{code/base_kernels.tex}
We sample a random subset S of
the set of supplied base kernels. S is of size $n \leq m$. We write
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
\Sbf = \{K_{\bm{\theta}^i}^i,\cdots,K_{\bm{\theta}^n}^n\}
\sim P(\Sbf = \{K_{\bm{\theta}^i}^i,\cdots,K_{\bm{\theta}^n}^n\} \mid
\text{BK}) 
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
with
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
P(\Sbf = \{K_{\bm{\theta}^i}^i,\cdots,K_{\bm{\theta}^n}^n\}\mid \text{BK}) =
\frac{n!}{ \mid \Sbf = \{K_{\bm{\theta}^i}^i,\cdots,K_{\bm{\theta}^n}^n\}\mid !}.
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

BK is assumed to be fixed as the most general margin of our hypothesis space.
In the following, we will drop it in the notation.
The only building block that we are now missing is how to combine the sampled
base kernels into a compositional covariance function (see Fig. \ref{fig:schema}
(b)). For each interaction $i$, we
have to infer whether the data supports a local interaction or a global interaction,
chosing between one out of two algebraic operators
$\bm{\Omega}_i=\{+,\times\}$. The probability for all such decisions is given by a binomial distribution: 
\begin{equation}
P(\bm{\Omega} \mid \Sbf)= {n \choose r}  p_{+\times}^r (1 - p_{+\times})^{n-r}.
\end{equation}
We can write the marginal probability of a kernel function as 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
P(\Krv \mid \xbf,\ybf,\thetabf ) = \iint \limits_{\bm{\Omega},\Sbf}
P(\Krv \mid \xbf,\ybf,\thetabf,\bm{\Omega},\Sbf) \times P(\bm{\Omega} \mid \Sbf)\times
P(\Sbf)\; \text{\bf d}\bm{\Omega}\, \text{\bf d}\Sbf\,
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
with $\bm{\theta}\subseteq \bm{\theta}^*$ as implied by $S$.
For structure learning with \ac{GP} kernels, a composite kernel is
sampled from $P(\Krv)$ and fed into \gpmem. 
The emulator generated by \gpmem\ observes unstructured time series data.
Venture code for the probabilistic grammar is shown in Listing
\ref{alg:grammar}, code for inference with \gpmem\ in Listing
\ref{alg:structureVent}. 


\input{code/grammar.tex}
\input{code/inference.tex}





Many equivalent covariance structures can be sampled due to covariance function algebra
and equivalent representations with different parameterization~\citep{lloyd2014automatic}.
To inspect the posterior of these equivalent structures we convert each kernel expression
into a sum of products and subsequently simplify. 
We introduce three different operators that work on kernel functions:
\begin{enumerate}
\item $\Simplify(k)$; this operators simplifies a kernel function $k$ according
to the simplifications that we present in Appendix B and Fig.
\ref{fig:schema} (b).
\item $\Struct(k)$; interprets the structure of a covariance function, for
example $\Struct(\klin)=\text{LIN}$; and
\item $\Parse(k)$, an operator that parses a covariance function. 
\end{enumerate}
All base kernels can be found in Appendix A,
rules for this simplification can be found in Appendix B.
Kernel structure comes with symbolic interpretations. To read kernel function $k$
and apply the simplifications described above, we apply, Struct$(k)$. For
examle, we write
\[
\Struct(\klin)=\text{LIN},
\]
which translates a function into a symbolic expression, see Appendix C. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%     Mauna result      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We defined a simple space of covariance structures in a way that allows us to produce results coherent with 
work presented in Automatic Statistician. The results are illustrated with two data sets.

\myparagraph{Mauna Loa  CO$_2$ data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/structure_posterior.tex}
\caption{\small Structure Learning. Starting with raw data (a), we fit a \ac{GP}
(b) and compute the posterior distribution on structures (c). We take a sample
of the peak of this distribution ($\text{LIN}+\text{PER}+\text{SE}+\text{WN}$)
including its parameters and write it in functional form (d). We depict the
human readable interpretation (e). We used (d) to plot (b).}\label{fig:posterior}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We illustrate results in Fig \ref{fig:posterior}. In Fig \ref{fig:posterior} (a) we depict the raw data. 
We see mean centered CO$_2$ measurements of the Mauna Loa Observatory, an atmospheric
baseline station on Mauna Loa, on the island of Hawaii. 
A description of the data set  can be found in  \citealp[][chapter 5]{rasmussen2006gaussian}.  
We use those raw data to compute a posterior on structure, parameters and \ac{GP}
samples.
The latter are shown in  Fig \ref{fig:posterior} (b)
where we zoom in to show how the posterior captures the error bars
adequately.
This posterior of the \ac{GP} is generated with a random sample from the parameters
of the peak of the distribution on structure (Fig \ref{fig:posterior} (c)).
We differentiate between a posterior distribution on kernel functions and on
distribution on symbolic expressions describing different kernel structures. 
This allows us to compute the posterior of symbollically equivalent structures,
such as $\Struct(\klin + \kper)=\Struct(\kper + \klin)$. Both structures yield and addition of a linear kernel and a periodic kernel, that is LIN + PER.
We coin the random variable over symbolic expressions for kernels as $\Ksrv$
which we compute with $\Simplify()$.
The distribution peaks at:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\Ksrv=\text{LIN} + \text{PER} + \text{SE} + \text{WN}.
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We write this kernel equation out in Fig \ref{fig:posterior} (d).
This kernel structure has a natural language interpretation that we spell out in
Fig \ref{fig:posterior} (e), explaining that 
the posterior peaks at a kernel structure with four additive components.
Each of which holds globally, that is there are no higher level, qualitative aspects
of the data that vary with the input space. The additive components for this result are as follows:
\begin{itemize}
\item a linearly increasing function or trend; 
\item a periodic function;
\item a smooth function; and
\item white noise.
\end{itemize}
 



Previous work on automated kernel discovery~\citep{duvenaud2013structure} illustrated the Mauna Loa data using an RQ kernel.
We resort to the white noise kernel instead of RQ (similar to \citep{lloyd2014automatic}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%     Airline result   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myparagraph{Airline Data}
The second data set (Fig. \ref{fig:posterior_airline}) we depict results for is  the airline 
data set describing monthly totals of international airline passengers (\citealp{box2011time}, according to \citealp{duvenaud2013structure}). 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/structure_posterior_airline.tex}
\caption{\small Structure Learning. Starting with raw data (a), we fit a \ac{GP}
(b) and compute the posterior distribution on structures (c). We take a sample
of the peak of this distribution ($\text{LIN}+\text{PER} \times \text{SE}+\text{WN}$)
including its parameters and write it in functional form (d). We depict the
human readable interpretation (e). We used (d) to plot (b).}\label{fig:posterior_airline}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We illustrate results for this data set in Fig \ref{fig:posterior_airline}. In Fig \ref{fig:posterior_airline} (a) we depict the raw data. 
Again, the data is mean centered and we use it to 
compute a posterior on structure, parameters and \ac{GP}
samples.
The latter are shown in  Fig \ref{fig:posterior_airline} (b).
This posterior of the \ac{GP} is generated with a random sample from the parameters
of the peak of the distribution on structure (Fig \ref{fig:posterior_airline} (c)).
The posterior over symbolic kernel expressions peaks at:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\Ksrv=\text{LIN} +  \text{SE} \times \text{PER}+ \text{WN}.
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We write this Kernel equation out in Fig \ref{fig:posterior_airline} (d).
This kernel structure has a natural language interpretation that we spell out in
Fig \ref{fig:posterior_airline} (e), explaining that 
the posterior peaks at a kernel structure with three additive components.
Additive components hold globally, that is there are no higher level, qualitative aspects
of the data that vary with the input space.
The additive components are as follows: 
\begin{itemize}
\item a linearly increasing function or trend;
\item a approximate periodic function; and
\item  white noise.
\end{itemize}
Both datasets served as illustrations in the Automatic Statistician project.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% Queries for time series %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myparagraph{Querying time series}
With our Bayesian approach to structure learning we can gain valuable insights
into time series data that were previously unavailable.
This is due to our ability to estimate posterior marginal probabilities over the kernel structure.
Over this marginal, we define boolean search operations that allow us to query the data
for the probability of certain structures to hold true globally.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\label{eq:bool_present}
P(\Ksrv \mid \xbf,\ybf,\thetabf) = \frac{1}{T}
\sum\limits_{t=1}^T f(\Ksrv^t)\;\;\text{where}\, f(\Ksrv^t) = \begin{cases}
  1, & \text{if } \Ksrv \underset{global}{\in} \Ksrv^t, \\
  0, & \text{otherwise}.
\end{cases} 
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
to ask whether it is true that a global structure $\Ksrv$ is present. $T$
is the number of all posterior samples for $\Ksrv$ and $\Ksrv^t$ is one such
sample. 
We can now ask simple questions, for example:
\begin{quotation}
Is there white noise in the data?
\end{quotation}
where we set $\Ksrv = $WN in (\ref{eq:bool_present}).
We can also formulate more sophisticated search operations using Boolean operators such as AND ($\land$) and OR ($\lor$).
The AND operator is defined as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[
P(\Ksrv^a \land \Ksrv^b \mid \xbf,\ybf, \thetabf)  = \frac{1}{N}
\sum\limits_{n=1}^N f(\Ksrv^t)\;\;
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where
\[
f(\Ksrv^t) = \begin{cases}
  1, & \text{if } \Ksrv^a\, \text{and}\, \Ksrv^b  \underset{global}{\in} \Ksrv^t, \\
  0, & \text{otherwise}\end{cases}.
\]
By estimating $P(\text{LIN} \land \text{WN} \mid \xbf, \ybf, \thetabf)$ we can use this operator to ask questions such as 
\begin{quotation}
Is there a Linear component AND a white noise in the data? 
\end{quotation}
Finally, we define the logical OR as
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
P(\Ksrv^a \lor \Ksrv^b \mid \xbf, \ybf, \thetabf)
=& P(\Ksrv^a \mid \xbf, \ybf, \thetabf) + P(\Ksrv^b \mid \xbf, \ybf, \thetabf)\\
 &- P(\Ksrv^a \land \Ksrv^b \mid \xbf, \ybf, \thetabf)
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
which allows us to ask questions about structures that are logically connected with OR, such as:
\begin{quotation}
Is there white noise or heteroskedastic noise?
\end{quotation}
by estimating $P(\text{LIN} \times \text{WN}\;\;{\large\lor}\;\; \text{WN} \mid
\xbf, \ybf, \thetabf)$.
We know that noise can either be heteroskedastic or white,
and we also know due to simple manipulations using kernel algebra
that  $\text{LIN} \times \text{WN}$ and $\text{WN}$ are the only possible ways to construct noise with kernel composition, we see that we can generalize the 
question above to:
\begin{quotation}
Is there noise in the data? 
\end{quotation}
where we write the marginal posterior on qualitative structure for noise:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
P(\Ksrv^{\text{noise}} \mid \xbf, \ybf, \thetabf) = P(\text{LIN} \times
\text{WN}\;\;{\large\lor}\;\; \text{WN} \mid \xbf, \ybf, \thetabf).
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Note that this allows us to start with general queries and 
subsequently formulate follow up queries that go into more detail.
For example, we could start with a general query, such as:
\begin{quotation}
What is the probability of a trend, a recurring pattern {\bf and} noise in the data?
\end{quotation}
and then follow up with more detailed questions (Fig \ref{fig:query}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\input{figs/structureQuery.tex}
\caption{{\bf Querying structural motifs in in time series using posterior inference
over kernel structure.} The kernel structure serves as a way to formulate
natural language questions about the data (blue). The initial question of interest
(top) is a fairly
general one: "What is the probability of a trend, a recurring
pattern and noise in the data?" Below the natural language version of this
question, the same question is formulated as an inference problem (black) over the
marginal probability on kernels with Boolean operators AND ($\land$) and OR ($\lor$). 
To gain  a deeper understanding of specific motifs in the time series more specific queries can
be written.
On the right, a query asks whether there is noise in the data (blue) by computing the disjunction of the marginal
of a global white noise kernel and a multiplication between a linear and a white
noise kernel (black). Samples from the predictive prior $\ybf_*$ of such kernels give an
indication of the qualitative aspects that a kernel structure implies (coloured curves below
the marginal). 
If the probability that there is noise in the data is high then it makes sense
to drill even deeper asking more detailed questions. With regards to noise, this
translates to querying whether or not the data supports the hypothesis that there is
heteroskedastic noise or white noise. Queries for motifs of repeating structure
are shown in the middle of the tree, queries related to trends on the left.}\label{fig:query}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This way of querying your data for their statistical implications is in stark contrast to what previous research in automatic kernel construction was able to provide.
We could view our approach as a time series search engine which allows us to test whether or not certain structures can be found
in an available time series.
Another way to view this approach is as a new language to interact with the world.
Real-world observations often come with time-stamps and in form
of continuous valued sensor measurements.  
We provide the toolbox to query such observations in a similar manner as
one would query a knowledge base in a logic programming language.







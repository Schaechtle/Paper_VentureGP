\label{sec:special-case-gpmem}
We implement \gpmem\ by memoizing a target procedure in a wrapper that remembers previously computed values.
This comes with interesting implications:
from the standpoint of computation, a data set of the form $\{(x_i, y_i)\}$ can be thought of as a function $y = f_\restr(x)$, where $f_\restr$ is restricted to only allow evaluation at a specific set of inputs $x$ (Alg. \ref{alg:frest}).
\begin{algorithm}

\caption{Restricted Function}

\begin{algorithmic}[1]

\Function{$f_\restr$}{$x$}
\If{$x \in \mathcal{D}$}
 \State \Return $\mathcal{D}[x]$
\Else
 \State raise Exception('Illegal input')
\EndIf 
\EndFunction

\end{algorithmic}
  \label{alg:frest}


\end{algorithm}
Modelling the data set with a \ac{GP} then amounts to trying to learn a smooth function $f_\emu$ (``emu'' stands for ``emulator'') which extends $f$ to its full domain.
Indeed, if $f_\restr$ is a foreign procedure made available as a black-box to Venture, whose secret underlying pseudo code is in Alg. \ref{alg:frest},
then the \texttt{observe} code can be rewritten using \gpmem\ as in Listing \ref{alg:venfrest} (where here the data set \texttt{D} has keys \texttt{x[1]},\ldots,\texttt{x[n]}):
\begin{mdframed}
\begin{minipage}{0.9\linewidth}
\small
\begin{lstlisting}[mathescape,label=alg:venfrest,basicstyle=\selectfont\ttfamily,numbers=none,caption={Observation with \gpmem},escapechar=\#]
#\linenumber{1}#assume (f_compute f_emu) =  gpmem( f_restr)
#\linenumber{2}#for i=1 to n:
#\linenumber{3}#  predict f_compute( x[i])
#\linenumber{4}#  infer mh(quote(hyper-parameters), one, 100)
#\linenumber{5}#sample (f_emu( array( 1, 2, 3))
\end{lstlisting}
\end{minipage}
\end{mdframed}


\vspace{0.2cm}
This rewriting has at least two benefits: (i) readability (in some cases), and (ii) amenability to active learning.
As to (i), the statistical code of creating a Gaussian process is replaced with a memoization-like idiom, which will be more familiar to programmers.
As to (ii), when using \gpmem, it is quite easy to decide incrementally which data point to sample next: for example, the loop from \texttt{x[1]} to \texttt{x[n]} could be replaced by a loop in which the next index \texttt{i} is chosen by a supplied decision rule.
In this way, we could use \gpmem\ to perform online learning using only a subset of the available data.

\begin{figure}
\input{figs/gpmem_tutorial.tex} 
%\put(-245,10){\line(1,0){200}}
\put(-40,98){\color{ForestGreen}\thicklines \vector(0,-1){20}}
\put(-117,11){\color{ForestGreen}\thicklines \vector(0,-1){20}}
\put(-59,-68){\thicklines \vector(0,-1){20}}
\put(-103,-43){\thicklines \vector(0,-1){20}}
\put(-91,-75){\thicklines \vector(0,1){20}}
\caption{\gpmem\ tutorial. The top shows a schematic of \gpmem. $f_{com} = $ \texttt{f\_compute} probes an outside resource. This can be expensive (top left). Every probe is memoized and improves the \ac{GP}-based emulator. Below the schematic we see a movie of the evolution of \gpmem's state of believe of the world given certain Venture directives.}
\end{figure}

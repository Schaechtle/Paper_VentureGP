\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{bm}
\usepackage{amsmath,amssymb}
\usepackage{natbib}
\usepackage{listings}
\usepackage[framemethod=TikZ]{mdframed}% http://ctan.org/pkg/mdframed
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}



\usetikzlibrary{pgfplots.groupplots}
%opening
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Gaussian Processes with Probabilistic Programming}


\author{
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Abstract
\end{abstract}
\section{Introduction}
MCMC lends itself to Bayesian interpretations of Gaussian Processes since they can provide a vehicle to express otherwise intractable integrals necessary for a fully Bayesian representation.

\section{Gaussian Processes}
In the following, we will introduce GP related theory and notations. We will exclusively work on two variable regression problems. Let the data be real-valued scalars  $\{x_i,y_i\}_{i=1}^n$ (complete data will be denoted by column vectors $\mathbf{x}$, $\mathbf{y}$). GPs present a non-parametric way to express prior knowledge on the space of possible functions  $f$ that we assume to have generated the data.  $f$ is assumed latent and the GP prior is given by a multivariate Gaussian with mean and covariance $f(\mathbf{x})\sim \mathcal{GP}(m(\mathbf{x}),k(\mathbf{x},\mathbf{x}'))$, where $m(\mathbf{x})$ is a function of the mean of all functions that map to $y_i$ at $x_i$ and $k(\mathbf{x},\mathbf{x}')$ is a kernel or covariance function that summarizes the covariance of all functions that map to $y_i$ at $x_i$. We can absorb the mean function into the covariance function so without loss of generality we can set the mean to zero. The marginal likelihood can be expressed as:
\begin{equation}
\label{eq:marg}
p(\mathbf{y}|\mathbf{x}) = \int p(\mathbf{y}|\mathbf{f,x})\, p(\mathbf{f}|\mathbf{x}) \, d\mathbf{f} 
\end{equation}
where the prior is Gaussian $\mathbf{f}|\mathbf{x} \sim \mathcal{N}\big(0,k(\mathbf{x},\mathbf{x}')\big)$. For a zero mean Gaussian Process this results in a Gaussian posterior $\mathcal{N}(\bm{\mu},\bm{\Sigma})$ with mean:
\begin{equation}
\label{eq:conditonalGaussianMean}
\bm{\mu} = \mathbf{K}(\mathbf{x},\mathbf{x}^*)\,\mathbf{K}(\mathbf{x}^*,\mathbf{x}^*)^{-1}\,\mathbf{y}
\end{equation}
and covariance
\begin{equation}
\label{eq:conditonalGaussianCovariance}
\bm{\Sigma} =  \mathbf{K}(\mathbf{x},\mathbf{x}) + \mathbf{K}(\mathbf{x},\mathbf{x}^*)\mathbf{K}(\mathbf{x}^*,\mathbf{x}^*)^{-1} \mathbf{K}(\mathbf{x}^*,\mathbf{x}).
\end{equation}
where $\mathbf{K}$ is a covariance function. The covariance function covers general high-level properties of the observed data such as linearity, periodicity and smoothness. The most widely used type of covariance function is the squared exponential covariance function:
\begin{equation}
k(x,x^\prime) = \sigma^2 \exp(-\frac{(x-x^\prime)^2}{2\ell^2})
\end{equation}
where $\sigma$ and $\ell$ are hyper-parameters. $\sigma$ is a scaling factor and $\ell$ is the typical length-scale.
Smaller variations can be achieved by exchanging these hyper-parameters. Below, we see how we can express simple GP smoothing with a few lines of Venture code while allowing users to custom design covariance functions. 
\begin{minipage}{\linewidth}
\belowcaptionskip=-10pt
\begin{lstlisting}[frame=single,label=alg:gpsmooth,caption=GP Smoothing,mathescape]
[ASSUME l 1]
[ASSUME sf 2]

$\smash{k(x,x^\prime) := \sigma^2 \exp(-\frac{(x-x^\prime)^2}{2\ell^2})}$

[ASSUME f VentureFunction($k,\sigma,\ell$) ]
[ASSUME SE make-se (apply-function f l sf) ]
[ASSUME (make-gp 0 SE ) ]
\end{lstlisting}
\end{minipage}


In the case where hyper-parameters are unknown they can be found deterministically by optimizing the marginal likelihood using a gradient based optimizer. Non-deterministic, Bayesian representations of this case are also known~\citep{neal1997monte}. Extending the program described in listing \ref{alg:gpsmooth} for a Bayesian treatment of hyper-parameters is simple using the build in stochastic procedure that simulates drawing samples from a gamma distribution:

\begin{minipage}{\linewidth}
\begin{lstlisting}[frame=single,label=alg:gpNeal,caption=Bayesian GP Smoothing,mathescape]
[ASSUME l (gamma 1 3)]
[ASSUME sf (gamma 1 2)]

$\smash{k(x,x^\prime) := \sigma^2 \exp(-\frac{(x-x^\prime)^2}{2\ell^2})}$

[ASSUME f VentureFunction($k,\sigma,\ell$) ]
[ASSUME SE make-se (apply-function f l sf) ]
[ASSUME (make-gp 0 SE ) ]
\end{lstlisting}
\end{minipage}


Larger variations are achieved by changing the type of the covariance function structure. A different type could be a linear covariance function:
\begin{equation}
 k(x,x^\prime) = \sigma^2 (x-\ell) (x^\prime-\ell). 
\end{equation}
Note that covariance function structures are compositional. We can add covariance functions if we want to model globally valid structures
\begin{equation}
k_3(x,x^\prime) = k_1(x,x^\prime) + k_2(x,x^\prime)
\end{equation}
 and we can multiply covariance functions if the data is best explained by local structure 
\begin{equation}
k_4(x,x^\prime) = k_1(x,x^\prime) \times k_2(x,x^\prime);
\end{equation}
both, $k_3$ and $k_4$ are valid covariance function structures. This leads to an infinite space of possible structures that could potentially explain the observed data best (e.g. Fig. \ref{fig:composite}). In the following, we will refer to covariance functions that are not composite as base covariance functions. Note that this form of composition can be easily expressed in Venture, for example if one wishes to add a linear and a periodic kernel:

\begin{minipage}{\linewidth}

\begin{lstlisting}[frame=single,label=alg:gpNeal,caption=LIN $\times$ PER,mathescape]
[ASSUME l (gamma 1 3)]
[ASSUME sf (gamma 1 2)]
[ASSUME a (gamma 2 2)]

$\smash{k_{LIN}(x,x^\prime) = \sigma_1^2 (x - \ell) (x^\prime - \ell)}$

$\smash{k_{PER}(x,x^\prime) := \sigma_2^2 \exp(-\frac{2 \sin^2(\pi(x-x^\prime)/p}{\ell^2})}$

[ASSUME f$_{LIN}$ VentureFunction($k_{LIN},\sigma_1$) ]
[ASSUME f$_{PER}$ VentureFunction($k_{PER},\sigma_2,\ell,p$) ]
[ASSUME LIN (make-LIN (apply-function f$_{LIN}$ a)) ]
[ASSUME PER (make-PER (apply-function f$_{PER}$ l sf)) ]
[ASSUME (make-gp 0 (function-times LIN PER)) ]
\end{lstlisting}

\end{minipage}




\begin{figure}[p]
\centering
    \input{figs/composition.tikz}\label{fig:composite}
    \caption{Composition of covariance functions (blue, left) and samples from the distribution of curves they can produce (red, right).}
\end{figure}

Knowledge about the composite nature of covariance functions is not new, however, until recently, the choice and the composition of covariance functions were done ad-hoc. The Automated Statistician Project came up with an approximate search over the possible space of kernel structures~\citep{duvenaud2013structure,lloyd2014automatic}. 

\subsection{A Bayesian interpretation}
In the following, we will explore a Bayesian representation of GP. The probability of the hyper-parameters of a GP with assumptions as above and given covariance function structure $\mathbf{K}$ can be described as:
\begin{equation}
\label{eq:hyperProbability}
P(\bm{\theta} \mid \mathbf{D,K}) = \frac{P(\mathbf{D} \mid \bm{\theta}, \mathbf{K})P(\bm{\theta} \mid  \mathbf{K})}{P(\mathbf{D} \mid \mathbf{K})}.
\end{equation}
We are interested in the case where covariance structure is not given. Our probabilistic programming based MCMC framework approximates the following intractable integrals of the expectation for the prediction:
\begin{equation}
\mathbb{E}[y^* \mid x^*,D,\mathbf{K}^s_{\Omega}] =\iint f(x^*,\bm{\theta},\mathbf{K})\,P(\bm{\theta} \mid \mathbf{D,\mathbf{K}})\,P(\mathbf{K}|\bm{\Omega},s,n) \; \mathbf{d} \bm{\theta} \mathbf{d} \mathbf{K}.  
\end{equation}
This is done by sampling from the posterior probability distribution of the hyper-parameters and the possible kernel:
\begin{equation}
y^* \approx \frac{1}{T} \sum^T_{t=1} f(x^* | \bm{\theta}^{(t)},\mathbf{K}^{(t)}). 
\end{equation}





\section{Stochastic Processes}
In order to provide the sampling of the kernel, we introduce a stochastic process to the SP that simulates the grammar for algebraic expressions of kernel algebra. Here, we start with a set of possible kernels and draw a random subset. For this subset of size $n$, we sample a set of possible operators that operate on the base kernels. 

The marginal probability of a kernel structure which allows us to sample  is characterized by the probability of a uniformly chosen subset of the set of $n$ possible covariance functions times the probability of sampling a global or a local structure which is given by a binomial distribution: 

\begin{equation}
P(\mathbf{K} \mid \bm{\Omega},s,n) = P(\bm{\Omega} \mid s,n)\times P(s \mid n) \times P(n),
\end{equation}
with
\begin{equation}
P(\bm{\Omega} \mid s,n)= {n \choose r}  p_{+\times}^k (1 - p_{+\times})^{n-k}
\end{equation}
and
\begin{equation}
P(s \mid n) = \frac{n!}{ \mid s \mid !}
\end{equation}
where $P(n)$ is a prior on the number of base kernels used. It is possible to also assign a prior for the probability to sample global or local priors, however, we have assigned complete uncertainty to this with the binomial $p = 0.5$.

\section{Experiments}

Neal suggested the treatment of outliers as a use-case for a Bayesian treatment of Gaussian processes~\citeyearpar{neal1997monte}. He evaluates his MCMC setting using the following synthetic data problem. Let $f$ be the underlying function that generates the data:
\begin{equation}
f(x) =  0.3 + 0.4 x + 0.5 \sin(2.7x) + \frac{1.1}{(1+ x^2)} + \eta \;\;\; with\;\;\eta \sim \mathcal{N}(0,\sigma)
\end{equation}
We synthetically generate outliers by setting $\sigma = 0.1$ in $95\%$ of the case and to $\sigma = 1$ in the remaining cases. Venture GPs can capture the true underlying function within only 100 MH steps (see Fig. \ref{fig:neal}). Note that Neal devices an additional noise model and performs large numbe of Hybrid-Monte Carlo and Gibbs steps.  

\begin{figure}
        \centering
        \begin{subfigure}[b]{0.3\textwidth} \centering
                \includegraphics[height=4.5cm]{figs/neal_se_1a.png}
                \caption{Prior}
                \label{fig:NealBO}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.3\textwidth} \centering
                \includegraphics[height=4.5cm]{figs/neal_se_2a.png}
                \caption{Observed}
                \label{fig:NealAO))}
        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.3\textwidth} \centering
                \includegraphics[height=4.5cm]{figs/neal_se_3a.png}
                \caption{Inferred}
                \label{fig:NealAI}
        \end{subfigure}
        \caption{Running a Venture GP on Neal's example for MCMC showing the prior, after having observed the data and after performing inference on the hyper-parameters. Note how the GP is choosing outliers to smooth instead of essential data. }\label{fig:neal}
\end{figure}


\subsection{Structure Learning}.
To inspect the posterior of the structure of the covariance function we convert each kernel expression into a sum of products and subsequently simplify expressions using the following grammar:
\begin{minipage}{\linewidth}

\begin{lstlisting}[frame=single,label=alg:simplify,caption=Grammar to simplify expressions,mathescape]
SE $\times$ SE                  $\rightarrow$ SE 
{SE,PER,C,WN} $\times$ WN       $\rightarrow$ WN
LIN $+$ LIN                $\rightarrow$ LIN
{SE,PER,C,WN,LIN} $\times$ C    $\rightarrow$  {SE,PER,C,WN,LIN} 
\end{lstlisting}

\end{minipage}



We defined a set of covariance structures so that we could reproduce results for covariance function structure learning as in the Automated Statistician. Our results are very similar to what has been reported by previous work (~\citealp*{duvenaud2013structure}; see Fig. \ref{fig:structure}).
\begin{figure}[p]

\centering
    \includegraphics[width=\textwidth]{figs/prelimCO2structure.png}
    \caption{Preliminary results from the cross-validation on the CO2 data. Note that \citet{duvenaud2013structure} report LIN $\times$ SE $+$ PER $\times$ SE $+$ RQ $\times$ SE. We have run a leave one out cross-validation on this data set. Above we see the preliminary results on 181 validations (of a total of 545 $\times$ 2 runs).}\label{fig:structure}
\end{figure}

\begin{figure}
        \centering
        \begin{subfigure}[b]{\textwidth} \centering
                \includegraphics[height=3cm]{figs/airline_sexper_1a.png}
                \caption{Prior, before having seen any data.}
                \label{fig:airlineBO}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
          
        \begin{subfigure}[b]{\textwidth} \centering
                \includegraphics[height=3cm]{figs/airline_sexper_2a.png}
                \caption{After having seen any data but before inference}
                \label{fig:AirlineA))}
        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
          
        \begin{subfigure}[b]{\textwidth} \centering
                \includegraphics[height=3cm]{figs/airline_sexper_3a.png}
                \caption{After 1000 MH steps.}
                \label{fig:airlineAI}
        \end{subfigure}
        \caption{Running a Venuter GP with covariance structure PER x SE on the airline data}\label{fig:airline}
\end{figure}

\subsection{Log-Likelihood}

%\begin{tcolorbox}
%       ripl.assume('l',"(tag (quote parameter) 0 (log (uniformcontinuous 0.01 8 )))")
%        ripl.assume('p',"(tag (quote parameter) 1 (log (uniform_continuous 0.1 8 )))")
%        ripl.assume('sf',"(tag (quote parameter) 2 (log (uniform_continuous 0.01 8 )))")
%        ripl.assume('gp',"""(tag (quote model) 0
                       %         (make_gp_part_der zero
                        %            (apply_function make_per l p sf )))""")

%\end{tcolorbox}
%\begin{figure}[p]
%\centering
%    \input{figs/underlyingFunctions.tikz}
%    \caption{We show how the log-Likelihood evolves over an MCMC chain for different test problems. We see how the correct %structures scores highest for each test problems while requiring few datapoint.}

%\end{figure}

\subsection{Residuals}
\bibliography{May2015}
\bibliographystyle{apalike}
\end{document}

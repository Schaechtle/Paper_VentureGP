\documentclass[twoside,11pt]{article}

% actual style
\usepackage{jmlr2e}

% utils
\usepackage[colorlinks=true,citecolor=MidnightBlue,urlcolor=blue]{hyperref} % remove for Arxiv
\usepackage{url}
\usepackage{natbib}
\usepackage[nolist]{acronym}
\usepackage{caption}
\usepackage{subcaption}

% plotting
\usepackage[svgnames,dvipsnames]{xcolor} % Specify colors by their 'svgnames'
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{pgfplots}
\usepackage[abs]{overpic}

% pseudo-code
\usepackage{algorithm}
\usepackage{algpseudocode}

% Venture code boxes
\input{venture_code_boxes.tex}
  
% math 
\usepackage{bm}
\usepackage{amsmath,amssymb}
\usepackage{mathtools}

% import new commands, colors and math operators
\input{new_commands}





% define acronyms here. Plural of words ending on s can be problematic.
\begin{acronym}
\acro{AAA} {absorb at applications}
\acro{GP} {Gaussian Processes}
\acro{MAP} {Maximum a posteriori}
\acro{MCMC} {Markov Chain Monte Carlo}
\acro{MDP} {Markov Decision Processes} 
\acro{MH} {Metropolis-Hastings}
\acro{PSP} {Primitive Stochastic Prodecure}
\acro{SP} {Stochastic Procedure}
\acro{SPI} {Stochastic Procedure Interface}
\acro{CCF} {Cosmic Calibration Framework}
\end{acronym}




% JMLR stuff - to be edited if accepted.
%\jmlrheading{}{}{}{}{}{}

% Short headings should be running head and authors last names

\ShortHeadings{Probabilistic Programming with Gaussian Processes}{}


\begin{document}
\title{Probabilistic Programming with Gaussian Process Memoization}


\author{\name Ulrich Schaechtle \email ulrich.schaechtle@rhul.ac.uk \\
	      \addr Department of Computer Science\\
              Royal Holloway, University of London
       \AND \name Ben Zinberg \email bzinberg@alum.mit.edu \\
              \addr Computer Science and Artificial Intelligence Laboratory\\
              Massachusetts Institute of Technology
       \AND \name Alexey Radul \email axch@mit.edu \\
              \addr Computer Science and Artificial Intelligence Laboratory\\
              Massachusetts Institute of Technology
       \AND \name Kostas Stathis \email kostas.stathis@rhul.ac.uk\\
              \addr Department of Computer Science\\
       Royal Holloway, University of London
       \AND \name Vikash K. Mansinghka \email vkm@mit.edu \\
	      \addr Computer Science and Artificial Intelligence Laboratory\\
	      Massachusetts Institute of Technology
} 

       \editor{N.A.}

\maketitle
\noindent\begin{abstract}
Gaussian Processes (GPs) are widely used tools in statistics, machine learning, robotics, computer vision, and 
scientific computation. However, despite their popularity, they can be difficult to apply; all but the simplest 
classification or regression applications require specification and inference over complex covariance functions that do 
not admit simple analytical posteriors.  Probabilistic programming shows potential for reducing the latter barrier,
if the computational surface of a GP can be suitably packaged for cooperating with available generic inference tactics.
This paper shows how to embed Gaussian processes in any higher-order 
probabilistic programming language, using an idiom based on memoization, and demonstrates its utility by implementing 
and extending classic and state-of-the-art GP applications. The interface to Gaussian processes, called \gpmem, takes an 
arbitrary real-valued computational process as input and returns a statistical emulator that automatically improves as 
the original process is invoked and its input-output behavior is recorded.  The flexibility of \gpmem\ is illustrated 
via three applications: (i) robust GP regression with hierarchical hyper-parameter learning, (ii) discovering symbolic 
expressions from time-series data by fully Bayesian structure learning over kernels generated by a stochastic grammar, 
and (iii) a bandit formulation of Bayesian optimization with automatic inference and action selection. All applications 
share a single 100-line Python library and require fewer than 20 lines of probabilistic code each.
\end{abstract}

\begin{keywords}
  Probabilistic Programming, Gaussian Processes, Structure Learning, Bayesian Optimization
\end{keywords}


\section{Introduction}
\input{sections/VenGPIntroduction.tex}
\setcounter{figure}{0}
\section{Background on Gaussian Processes}
\input{sections/GP.tex}

\subsection{Packaging Gaussian Processes for Venture}
\input{sections/GPinterface.tex}
\section{Gaussian Process Memoization in Venture}
\input{sections/gpmem.tex}

\section{Applications}
This paper illustrates the flexibility of \gpmem\ by showing how it can concisely encode three different applications of \ac{GP}s.
The first is a standard example from hierarchical Bayesian statistics, where Bayesian inference over a hierarchical hyper-prior is used to provide a curve-fitting methodology that is robust to outliers.
The second is a structure learning application from probabilistic artificial intelligence, where \ac{GP}s are used to discover qualitative structure in time series data.
The third is a reinforcement learning application, where \ac{GP}s are used as part of a Thompson sampling formulation of Bayesian optimization for general real-valued objective functions with real inputs.

\subsection{Nonlinear regression in the presence of outliers}
\input{sections/hyperparameters.tex}
\subsection{Discovering qualitative structure from time series data}\label{sec:structurelearning}
\input{sections/StructureLearning.tex}

\subsection{Bayesian optimization}

\label{sec:thompson}



\input{sections/Bayesian_Optimization_using_TS.tex}
\section{Discussion}
\input{sections/Conclusion.tex}
\myparagraph{Acknowledgements}
This research was supported by DARPA
  (under the XDATA and PPAML programs), IARPA (under research contract
  2015-15061000003), the Office of Naval Research (under research
  contract N000141310333), the Army Research Office (under agreement
  number W911NF-13-1-0212), the Bill \& Melinda Gates Foundation, and
  gifts from Analog Devices and Google.
\newpage
\section*{Appendix}
\subsection*{A Covariance Functions}
\input{sections/Appendix_Cov.tex}

\subsection*{B Covariance Simplification}
\input{sections/Appendix_Simplification.tex}

\subsection*{C The Struct-Operator}
\input{sections/Appendix_Struct.tex}

\newpage
\subsection*{D Glossary}
\input{sections/glossary.tex}
%\subsection*{C Additional Structure Learning Results}
%\input{sections/Appendix_Structure_Results.tex}

\newpage
\bibliography{VentureGP.01}
\bibliographystyle{apalike}
\end{document}
